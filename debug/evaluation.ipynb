{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"../../\")\n",
    "from myutils import *\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch    \n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "#   TESTING MODELS\n",
    "# -----------------------------------------------\n",
    "def enc_as_str(arr):\n",
    "    return enc_str_fr_np(arr,sep=None,br=None)\n",
    "\n",
    "def dec_as_np(string):\n",
    "    return dec_np_fr_str(string,sep=None,br=None)\n",
    "\n",
    "def test_iteration(        \n",
    "        name    = None,\n",
    "        random_state = None,\n",
    "        X       = None,\n",
    "        y       = None,\n",
    "        X_train = None,\n",
    "        y_train = None,\n",
    "        X_test  = None,\n",
    "        y_test  = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "    return PseudoObject(\n",
    "        name    = name,\n",
    "        random_state = random_state,\n",
    "        X       = X,\n",
    "        y       = y,\n",
    "        X_train = X_train,\n",
    "        y_train = y_train,\n",
    "        X_test  = X_test,\n",
    "        y_test  = y_test,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def test_iteration_results(\n",
    "        y_true      =None,\n",
    "        y_pred      =None,\n",
    "        dataset     =None,\n",
    "        random_state=None,\n",
    "        model       =None,\n",
    "        args        =None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "    return PseudoObject(\n",
    "        y_true      =y_true,\n",
    "        y_pred      =y_pred,\n",
    "        dataset     =dataset,\n",
    "        random_state=random_state,\n",
    "        model       =model,\n",
    "        args        =args,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def test_ilocs(\n",
    "        dataset     = None,\n",
    "        random_state= None,\n",
    "        labels      = None,\n",
    "        total_cnt   = None,\n",
    "        train_cnt   = None,\n",
    "        test_cnt    = None,\n",
    "        train_rows_iloc = None,\n",
    "        test_rows_iloc  = None,\n",
    "        train_cols_iloc = None,\n",
    "        test_cols_iloc  = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "    return PseudoObject(\n",
    "        dataset     = dataset,\n",
    "        random_state= random_state,\n",
    "        labels      = labels,\n",
    "        total_cnt   = total_cnt,\n",
    "        train_cnt   = train_cnt,\n",
    "        test_cnt    = test_cnt,\n",
    "        train_rows_iloc = train_rows_iloc,\n",
    "        test_rows_iloc  = test_rows_iloc,\n",
    "        train_cols_iloc = train_cols_iloc,\n",
    "        test_cols_iloc  = test_cols_iloc,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def run_test_iteration(test_iter, model_iter, return_trained_model=False):\n",
    "    # Retrieve the dataset\n",
    "    X_train,X_test,y_train,y_test,random_state,name = test_iter.values(['X_train','X_test','y_train','y_test','random_state','name'])\n",
    "\n",
    "    # Initialize our model\n",
    "    model_init, args = model_iter\n",
    "    if 'random_state' in get_args(model_init.__init__): # check if model accepts random_state keyword\n",
    "        model = model_init(random_state=random_state,**args)\n",
    "    else:\n",
    "        model = model_init(**args)\n",
    "\n",
    "    # Execute our test iteration and record the time elapsed\n",
    "    time_elapsed,model = ProcessTimer().execute(\n",
    "        job_id=0,         \n",
    "        return_val=True,\n",
    "        func=model.fit, \n",
    "        X=X_train, \n",
    "        y=y_train,\n",
    "    )\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = y_test\n",
    "\n",
    "    # Save our test result\n",
    "    test_iter_result = test_iteration_results(\n",
    "        dataset = name,\n",
    "        random_state = random_state,\n",
    "        model = type(model).__name__,\n",
    "        args = str(args),\n",
    "        y_true = enc_as_str(ravel(y_true,dtype=int)), # Encode the results as str instead of array\n",
    "        y_pred = enc_as_str(ravel(y_pred,dtype=int)), \n",
    "        time_elapsed = np.round(time_elapsed,4),\n",
    "    )\n",
    "\n",
    "    if return_trained_model:\n",
    "        return test_iter_result, model\n",
    "    else:\n",
    "        return test_iter_result\n",
    "\n",
    "\n",
    "def run_test_batch(dataset_obj,random_states,fsl_samples,model_batch,input_flag=0,write_to=None,save_test_ilocs=True):\n",
    "    test_batch_result = pd.DataFrame()\n",
    "    test_ilocs = pd.DataFrame()\n",
    "    for random_state in random_states:\n",
    "        # Test each model+kwargs combination and record the result \n",
    "        for model_iter in model_batch:\n",
    "            for n_samples in fsl_samples: #ravel() to prevent bugs\n",
    "                test_iter = create_test_iteration(dataset_obj,n_samples,random_state,input_flag)\n",
    "                test_iter_result = run_test_iteration(test_iter,model_iter)\n",
    "                test_batch_result = pd.concat([test_batch_result,test_iter_result.to_frame(dtype=\"object\")],ignore_index=True)  # Append result to DataFrame, converted to \"object\" first to avoid turning string into int\n",
    "                write_dataframe(test_batch_result,write_to)\n",
    "\n",
    "                test_iloc = create_test_iloc(test_iter)\n",
    "                test_ilocs = pd.concat([test_ilocs,test_iloc.to_frame(dtype=\"object\")],ignore_index=True)\n",
    "\n",
    "    write_dataframe(test_batch_result,write_to)\n",
    "    if save_test_ilocs:\n",
    "        write_dataframe(test_ilocs.drop_duplicates(),f\"{dir_to_file(write_to)}/test_iloc_table.csv\")\n",
    "\n",
    "    return test_batch_result\n",
    "\n",
    "\n",
    "def create_test_iloc(test_iter):\n",
    "    labels,counts = label_counts(test_iter.y).values([\"labels\",\"counts\"])  \n",
    "    test_iloc = test_ilocs(\n",
    "        dataset = test_iter.name,\n",
    "        random_state= test_iter.random_state,\n",
    "        labels = enc_str_fr_np(labels),\n",
    "        total_cnt = enc_str_fr_np(counts),\n",
    "        train_cnt = enc_str_fr_np(label_counts(test_iter.y_train,labels).counts),\n",
    "        test_cnt = enc_str_fr_np(label_counts(test_iter.y_test,labels).counts),\n",
    "        train_rows_iloc = enc_str_fr_np(get_array_iloc(test_iter.y_train.index,test_iter.y.index),sep=\" \",br=None),\n",
    "        test_rows_iloc = enc_str_fr_np(get_array_iloc(test_iter.y_test.index,test_iter.y.index),sep=\" \",br=None),\n",
    "        train_cols_iloc = enc_str_fr_np(get_array_iloc(test_iter.X_train.columns,test_iter.X.columns),sep=\" \",br=None),\n",
    "        test_cols_iloc = enc_str_fr_np(get_array_iloc(test_iter.X_test.columns,test_iter.X.columns),sep=\" \",br=None),\n",
    "    )\n",
    "    return test_iloc\n",
    "\n",
    "\n",
    "def load_test_iloc(test_iloc):\n",
    "    return test_ilocs(\n",
    "        dataset = test_iloc.loc['dataset'],\n",
    "        random_state= test_iloc.loc['random_state'],\n",
    "        labels = dec_np_fr_str2(test_iloc.loc['labels'],sep=\",\",dtype=int),\n",
    "        total_cnt = dec_np_fr_str2(test_iloc.loc['total_cnt'],sep=\",\",dtype=int),\n",
    "        train_cnt = dec_np_fr_str2(test_iloc.loc['train_cnt'],sep=\",\",dtype=int),\n",
    "        test_cnt = dec_np_fr_str2(test_iloc.loc['test_cnt'],sep=\",\",dtype=int),\n",
    "        train_rows_iloc = dec_np_fr_str2(test_iloc.loc['train_rows_iloc'],sep=\" \",br=None,dtype=int),\n",
    "        test_rows_iloc = dec_np_fr_str2(test_iloc.loc['test_rows_iloc'],sep=\" \",br=None,dtype=int),\n",
    "        train_cols_iloc = dec_np_fr_str2(test_iloc.loc['train_cols_iloc'],sep=\" \",br=None,dtype=int),\n",
    "        test_cols_iloc = dec_np_fr_str2(test_iloc.loc['test_cols_iloc'],sep=\" \",br=None,dtype=int),\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------\n",
    "#   SCORING MODELS\n",
    "# -----------------------------------------------\n",
    "def test_iteration_score(y_true,y_pred,pos_label=1,average=\"auto\"):\n",
    "    # Calculating the scores\n",
    "    true_counts,true_labels,true_n_class = label_counts(y_true).values(['counts','labels','num_classes'])\n",
    "    pred_counts = label_counts(y_pred,labels=true_labels).counts # Coerce the counts to be in the same order as the true counts\n",
    "    mcm = multilabel_confusion_matrix(y_true,y_pred,labels=true_labels)\n",
    "    corr_counts = np.array([mcm[i,-1,-1] for i in range(true_n_class)])\n",
    "\n",
    "    if true_n_class == 2:\n",
    "        average = \"binary\" if average == \"auto\" else average\n",
    "        conf_matrix = confusion_matrix(y_true,y_pred,labels=true_labels).ravel()\n",
    "    else:\n",
    "        average = \"macro\" if average == \"auto\" else average\n",
    "        conf_matrix = confusion_matrix(y_true,y_pred,labels=true_labels)\n",
    "\n",
    "    precision,recall,fscore,support = precision_recall_fscore_support(y_true,y_pred,pos_label=pos_label,average=average,zero_division=0)\n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "\n",
    "    # Save the score\n",
    "    # test_iter_score = test_iter_result.drop(['y_true','y_pred']).copy()\n",
    "    test_iter_score = pd.Series()\n",
    "    test_iter_score.loc['test_labels'] = true_labels\n",
    "    test_iter_score.loc['test_counts'] = true_counts\n",
    "    test_iter_score.loc['pred_counts'] = pred_counts\n",
    "    test_iter_score.loc['corr_counts'] = corr_counts\n",
    "    test_iter_score.loc['conf_matrix'] = conf_matrix\n",
    "    test_iter_score.loc['accuracy'] = np.round(accuracy,4)\n",
    "    test_iter_score.loc['precision'] = np.round(precision,4)\n",
    "    test_iter_score.loc['recall'] = np.round(recall,4)\n",
    "    test_iter_score.loc['fscore'] = np.round(fscore,4)\n",
    "    test_iter_score.loc['average'] = average\n",
    "\n",
    "    # Added later\n",
    "    tpr = corr_counts/true_counts\n",
    "    f1_binary = f1_score(y_true,y_pred,pos_label=pos_label,average=\"binary\") if true_n_class == 2 else None\n",
    "    f1_micro = f1_score(y_true,y_pred,pos_label=pos_label,average=\"micro\")\n",
    "    f1_macro = f1_score(y_true,y_pred,pos_label=pos_label,average=\"macro\")\n",
    "    f1_weighted = f1_score(y_true,y_pred,pos_label=pos_label,average=\"weighted\")\n",
    "    test_iter_score.loc['f1_binary'] = np.round(f1_binary,4) if not isNone(f1_binary) else f1_binary\n",
    "    test_iter_score.loc['f1_micro'] = np.round(f1_micro,4)\n",
    "    test_iter_score.loc['f1_macro'] = np.round(f1_macro,4)\n",
    "    test_iter_score.loc['f1_weighted'] = np.round(f1_weighted,4)\n",
    "    for i in true_labels:\n",
    "        test_iter_score.loc[f'tpr[{i}]'] = tpr[i]\n",
    "\n",
    "    return test_iter_score\n",
    "\n",
    "def test_iteration_score_from_result(test_iter_result,pos_label=1,average=\"auto\"):\n",
    "    if isinstance(test_iter_result,PseudoObject):\n",
    "        test_iter_result = test_iter_result.to_series()\n",
    "    \n",
    "    y_true = dec_np_fr_str2(test_iter_result['y_true'],sep=\"\",dtype=int)\n",
    "    y_pred = dec_np_fr_str2(test_iter_result['y_pred'],sep=\"\",dtype=int)\n",
    "\n",
    "    test_iter_result = test_iter_result.drop(['y_true','y_pred']).copy()\n",
    "    test_iter_score = test_iteration_score(y_true,y_pred,pos_label=pos_label,average=average)\n",
    "    return pd.concat([test_iter_result,test_iter_score],axis=0)\n",
    "\n",
    "\n",
    "def test_batch_score_from_result(test_batch_result,write_to=None,**kwargs):\n",
    "    if isinstance(test_batch_result,str):\n",
    "        test_batch_result = read_dataframe(test_batch_result,dtype=\"object\") # Avoid converting y_true and y_pred to integers, which would remove leading 0s\n",
    "\n",
    "    test_batch_score_ = pd.DataFrame()\n",
    "    for i in range(len(test_batch_result)):\n",
    "        test_iter_result = test_batch_result.iloc[i]\n",
    "        test_iter_score = test_iteration_score_from_result(test_iter_result,**kwargs).to_frame().T\n",
    "        test_batch_score_ = pd.concat([test_batch_score_,test_iter_score],ignore_index=True)\n",
    "    write_dataframe(test_batch_score_,write_to)\n",
    "    return test_batch_score_\n",
    "\n",
    "def read_test_iter_scores(test_iter_score_str):\n",
    "    test_iter_score = test_iter_score_str.copy()\n",
    "    test_iter_score['test_labels'] = dec_np_fr_str2(test_iter_score_str['test_labels'],dtype=int)\n",
    "    test_iter_score['test_counts'] = dec_np_fr_str2(test_iter_score_str['test_counts'],dtype=int)\n",
    "    test_iter_score['pred_counts'] = dec_np_fr_str2(test_iter_score_str['pred_counts'],dtype=int)\n",
    "    test_iter_score['corr_counts'] = dec_np_fr_str2(test_iter_score_str['corr_counts'],dtype=int)\n",
    "    test_iter_score['conf_matrix'] = dec_np_fr_str2(test_iter_score_str['conf_matrix'],dtype=int,levels=2)\n",
    "    test_iter_score['precision'] = dec_np_fr_str2(test_iter_score_str['precision'],dtype=float)\n",
    "    test_iter_score['recall'] = dec_np_fr_str2(test_iter_score_str['recall'],dtype=float)\n",
    "    test_iter_score['fscore'] = dec_np_fr_str2(test_iter_score_str['fscore'],dtype=float)\n",
    "    return test_iter_score\n",
    "\n",
    "def read_scores_file(filename,**kwargs):\n",
    "    df = read_dataframe(filename,**kwargs)\n",
    "    test_batch_scores = pd.DataFrame()\n",
    "    for i in df.index:\n",
    "        test_iter_score = read_test_iter_scores(df.loc[i]).to_frame().T\n",
    "        test_batch_scores = pd.concat([test_batch_scores,test_iter_score],axis=0)\n",
    "    return test_batch_scores\n",
    "\n",
    "\n",
    "def read_results_file(filename):\n",
    "    return read_dataframe(filename)\n",
    "\n",
    "\n",
    "def plot_scores(title=None,**kwargs):\n",
    "    ax = sns.lineplot(**kwargs)\n",
    "    ax.set(title=title)\n",
    "    ax.grid(True,axis='y')\n",
    "    ax.legend(loc='lower center',bbox_to_anchor=(0.5,-0.8))\n",
    "\n",
    "def create_datasets(directory,**kwargs):\n",
    "    for key in kwargs.keys():\n",
    "        if key == \"description\":\n",
    "            write_file(kwargs[key],f\"{directory}/description.txt\")\n",
    "        else:\n",
    "            write_dataframe(kwargs[key],f\"{directory}/{key}.csv\")\n",
    "\n",
    "def load_datasets(directory,keys=[\"data\",\"X\",\"y\",\"metadata\",\"description\",\"X_train\",\"X_test\",\"y_train\",\"y_test\",\"test_iloc_table\"],**kwargs):\n",
    "    dataset_obj = PseudoObject(name=directory)\n",
    "    for key in keys:\n",
    "        if key == \"description\":\n",
    "            filename = f\"{directory}/{key}.txt\"\n",
    "            if os.path.isfile(filename):\n",
    "                dataset_obj.update(**{key:read_file(filename)})\n",
    "        else:\n",
    "            filename = f\"{directory}/{key}.csv\"\n",
    "            if os.path.isfile(filename):\n",
    "                dataset_obj.update(**{key:read_dataframe(filename,**kwargs)})\n",
    "    return dataset_obj"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
