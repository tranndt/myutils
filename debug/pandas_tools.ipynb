{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"../../\")\n",
    "from myutils import *\n",
    "#----------------------------------------\n",
    "#   READ AND WRITE DATAFRAMES\n",
    "#----------------------------------------\n",
    "\n",
    "def write_dataframe(df,write_to=None,**kwargs):\n",
    "    \"\"\"\n",
    "    Write the dataframe according to the extension. Create the new path if necessary\n",
    "    \"\"\"\n",
    "    if not isNone(write_to) and isinstance(write_to,str):\n",
    "        directory = makedir_to_file(write_to)\n",
    "        ext = write_to.split(\".\")[-1] # get the extension\n",
    "        if ext == 'xlsx':\n",
    "            df.to_excel(write_to,**kwargs)\n",
    "        elif ext in ['tsv','tab']:\n",
    "            df.to_csv(write_to,sep='\\t',**kwargs)\n",
    "        else:\n",
    "            df.to_csv(write_to,**kwargs)\n",
    "    return df\n",
    "\n",
    "def read_dataframe(filename,index=None,unnamed_col=False,as_series=False,**kwargs):\n",
    "    ext = filename.split(\".\")[-1] # get the extension\n",
    "    if ext == 'xlsx':\n",
    "        df = pd.read_excel(filename,**kwargs)\n",
    "    elif ext in ['tsv','tab']:\n",
    "        df = pd.read_csv(filename,sep='\\t',**kwargs)\n",
    "    else:\n",
    "        df = pd.read_csv(filename,**kwargs)\n",
    "\n",
    "    # Set index column if not None\n",
    "    if not isNone(index) and index in df.columns:\n",
    "        df = df.set_index(index)\n",
    "\n",
    "    # Drop the typical 'Unnamed: 0' from the \n",
    "    UNNAMED = \"Unnamed: 0\"\n",
    "    if UNNAMED in df.columns:\n",
    "        if unnamed_col == True:\n",
    "            pass\n",
    "        elif unnamed_col == False:\n",
    "            df = df.drop(columns=[UNNAMED])\n",
    "        elif isinstance(unnamed_col,(str,int,float)):\n",
    "            if unnamed_col in df.columns:\n",
    "                unnamed_col = f\"index: {unnamed_col}\"\n",
    "            df = df.rename(columns={UNNAMED:unnamed_col}) \n",
    "    if as_series:\n",
    "        return df.iloc[:,-1]\n",
    "    else:\n",
    "        return df\n",
    "       \n",
    "#----------------------------------------\n",
    "#   EXTRACT DATA FRAME INFORMATION\n",
    "#----------------------------------------\n",
    "def dataframe_like(ref,data=None,index=None,columns=None): \n",
    "    \"\"\"\n",
    "    Create a DataFrame/Series object that has (a subset of) the same index/columns as a reference DataFrame/Series\n",
    "    \"\"\"\n",
    "    res = ref\n",
    "    if isinstance(ref,pd.DataFrame):\n",
    "        index = isNone(index,then=ref.index)\n",
    "        columns = isNone(columns,then=ref.columns)\n",
    "        if isinstance(data,Iterable):\n",
    "            data = np.array(data)\n",
    "            index = index[:data.shape[0]]\n",
    "            columns = columns[:data.shape[1]]\n",
    "        res = pd.DataFrame(data=data,index=index,columns=columns)\n",
    "\n",
    "    elif isinstance(ref,pd.Series):\n",
    "        index = isNone(index,then=ref.index)\n",
    "        if isinstance(data,Iterable):\n",
    "            data = np.array(data)\n",
    "            index = index[:data.shape[0]]\n",
    "        res = pd.Series(data=data,index=index)\n",
    "    return res\n",
    "\n",
    "\n",
    "def summary(df:pd.DataFrame,nan={}) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summary table for each columns in the dataframe\n",
    "    \"\"\"\n",
    "    df_sum = pd.DataFrame(index=df.columns,columns=[\"dtypes\",\"length\",\"unique\",\"samples\",\"samples_pct\",\"nan_value\",\"mode\",\"range\",\"mean\",\"std\",\"fill\"])\n",
    "    df_sum.index.name = \"columns\"\n",
    "    for c in df.columns:\n",
    "        samples_cnts = df[c].value_counts(ascending=False,dropna=False).iloc[:5]\n",
    "        df_sum.loc[c,\"dtypes\"] = dtype(df[c])\n",
    "        df_sum.loc[c,\"samples\"] = (list(samples_cnts.index.values))\n",
    "        df_sum.loc[c,\"samples_pct\"] = (list(np.round(samples_cnts.values/len(df[c])*100,2)))\n",
    "        df_sum.loc[c,\"length\"] = len(df[c])\n",
    "        df_sum.loc[c,\"unique\"] = len(df[c].unique())\n",
    "        df_sum.loc[c,\"mode\"] = [] if len(df[c].mode()) == 0 else ravel(df[c].mode())[0]\n",
    "\n",
    "        if isinstance(nan,dict) and c in nan.keys():\n",
    "            nan_value = nan[c]\n",
    "            df_c_notna = df[c][df[c] != nan_value]\n",
    "        elif type_of(nan).startswith((\"int\",\"float\")):\n",
    "            nan_value = nan\n",
    "            df_c_notna = df[c][df[c] != nan_value]\n",
    "        else:\n",
    "            nan_value = np.nan\n",
    "            df_c_notna = df[c][df[c].notna()]\n",
    "\n",
    "        df_sum.loc[c,\"nan_value\"] = nan_value\n",
    "        df_sum.loc[c,\"fill\"] = np.round(len(df_c_notna)/len(df[c]),2)\n",
    "        \n",
    "        if dtype(df[c]).startswith((\"int\",\"float\")):\n",
    "            df_sum.loc[c,\"range\"] = np.round([df_c_notna.min(),df_c_notna.max()],4)\n",
    "            df_sum.loc[c,\"mean\"] = np.round(df_c_notna.mean(),4)\n",
    "            df_sum.loc[c,\"std\"] = np.round(df_c_notna.std(),4)\n",
    "    return df_sum\n",
    "\n",
    "def compare(df1 : pd.DataFrame or pd.Series, df2: pd.DataFrame or pd.Series,numeric: bool=False) -> pd.DataFrame or pd.Series:\n",
    "    \"\"\"\n",
    "    Compare the 2 DataFrames along the same indexes and columns. \n",
    "        - For numeric columns, return the difference. \n",
    "        - For object columns, return whether the values match\n",
    "    \"\"\"\n",
    "    overlapped_index = overlap(df1.index,df2.index)\n",
    "    overlapped_columns = overlap(df1.columns,df2.columns)\n",
    "    index = df1.index if set(df1.index) == set(overlapped_index) else overlapped_index\n",
    "    columns = df1.columns if set(df1.columns) == set(overlapped_columns) else overlapped_columns\n",
    "    df1_comp = df1.loc[index,columns]\n",
    "    df2_comp = df2.loc[index,columns]\n",
    "    df_comp = pd.DataFrame(index=index,columns=columns)\n",
    "\n",
    "    if numeric:\n",
    "        numeric_cols = []\n",
    "        for c in columns:\n",
    "            if is_numeric(df1[c]) and is_numeric(df2[c]): \n",
    "                numeric_cols.append(c)\n",
    "                df_comp.loc[index,c] = (df1_comp.loc[index,c] - df2_comp.loc[index,c])\n",
    "        return df_comp.loc[index,numeric_cols]\n",
    "\n",
    "    else:\n",
    "        for c in columns:\n",
    "            df_comp.loc[index,c] = (df1_comp.loc[index,c] == df2_comp.loc[index,c]) | ((df1_comp.loc[index,c] != df2_comp.loc[index,c]) \n",
    "                                                                                        & (df1_comp.loc[index,c].isna() == df2_comp.loc[index,c].isna())\n",
    "                                                                                        & df1_comp.loc[index,c].isna() \n",
    "                                                                                        & df2_comp.loc[index,c].isna())\n",
    "        return df_comp.loc[index,columns]\n",
    "\n",
    "#----------------------------------------\n",
    "# DATA FRAME MANIPULATION & TRANSFORMATION\n",
    "#----------------------------------------\n",
    "\n",
    "def filter(df: pd.DataFrame or pd.Series, condition: pd.DataFrame or pd.Series, filter_columns: bool=False) -> pd.DataFrame or pd.Series:\n",
    "    \"\"\"\n",
    "    @Description: Filter a `DataFrame` object based on a match over one or multiple columns. Results can be filtered by rows or both rows and columns.\n",
    "    @Parameters:\n",
    "        - condition: boolean Series, typically a DataFrame expression involving one or more conditions. E.g., `df['A'] == 1` or `df[['B','C']] == [2,3]`\n",
    "        - filter_columns: When filter_columns is False, only filter by rows, otherwise filter by both rows and columns\n",
    "    \"\"\"\n",
    "    df_ = pd.DataFrame(df)\n",
    "    filter_ = pd.DataFrame(condition)\n",
    "    # Ensure that condition works over multiple columns matching\n",
    "    match_all_columns = (pd.DataFrame(condition).sum(axis=1) == len(filter_.columns.values))\n",
    "    condition =  match_all_columns if len(filter_.columns.values) >= 1 else condition \n",
    "\n",
    "    # Select columns to keep and rows to display based on condition\n",
    "    columns_filt = filter_.columns.values if filter_columns else df_.columns.values\n",
    "    index_filt = df_.loc[:,columns_filt][condition].dropna().index    \n",
    "    return df_.loc[index_filt,columns_filt]\n",
    "\n",
    "def per_class_sample(X: pd.DataFrame, y: pd.DataFrame or pd.Series, \n",
    "                        sampling_dist: str or int or float or Iterable='min',random_state: int=None) -> Tuple[pd.DataFrame or pd.Series, pd.DataFrame or pd.Series]:\n",
    "    \"\"\"\n",
    "    ### Description: \n",
    "    Sample inputs based on a distribution of target labels. By default will attempt to sample all classes equally according to the least populous class.\n",
    "\n",
    "    ### Parameters:\n",
    "    - sampling_dist:\n",
    "        - If sampling_dist is `None`: Sample all classes .\n",
    "        - If sampling_dist is `min`: Attempt to sample all classes equally according to the least populous class.\n",
    "        - If sampling_dist is type `int`: Attempt to sample all classes up to a maximum of label_dists\n",
    "        - If sampling_dist is type `float` (within (0,1)): Attempt to sample all classes each with the proportion of label_dists\n",
    "        - If sampling_dist is type `list`: Attempt to sample classes based on the distribution specified.\n",
    "            - If a class distribution is `None`, all members of that class is sampled\n",
    "            - If a class distribution is a fraction (within (0,1)), it will be understood as the class proportion\n",
    "            - If a class distribution is an integer (>=1), it will be understood as class counts\n",
    "    \"\"\"\n",
    "    # Convert sampling_dist into list of distribution if not already is\n",
    "    counts,labels = label_counts(y).values(['counts','labels'])\n",
    "    if isNone(sampling_dist):\n",
    "        sampling_dist = counts\n",
    "    if isinstance(sampling_dist,str) and (sampling_dist == 'min'):\n",
    "        sampling_dist = min(counts)\n",
    "    if isinstance(sampling_dist,(int,float,np.int64,np.float64,np.int32,np.float32,np.int16,np.float16)):\n",
    "        sampling_dist = np.full(shape = labels.shape,fill_value = sampling_dist)\n",
    "\n",
    "    sampled_index = pd.Index([])\n",
    "    # sampled_iloc = []\n",
    "    for labels_i,counts_i, sampling_dist_i in zip(labels,counts,sampling_dist):\n",
    "        # Convert distribution values to actual class counts\n",
    "        if isNone(sampling_dist_i): \n",
    "            dist_i = int(counts_i)\n",
    "        elif 0 <= sampling_dist_i and sampling_dist_i < 1:\n",
    "            dist_i = int(counts_i * sampling_dist_i)\n",
    "        else:\n",
    "            dist_i = int(clamp(sampling_dist_i,0,counts_i))\n",
    "        # Obtain samples with the labels_i \n",
    "        sampled_targets_i = filter(y,y==labels_i).sample(dist_i,random_state=random_state)\n",
    "        sampled_index = sampled_index.append(sampled_targets_i.index)\n",
    "    return X.loc[sampled_index], y.loc[sampled_index]\n",
    "\n",
    "    #     sampled_iloc.append(get_array_iloc(sampled_targets_i.index,y.index))\n",
    "    # return X.iloc[sampled_iloc], y.iloc[sampled_iloc]\n",
    "\n",
    "\n",
    "def match(df_in: pd.DataFrame or pd.Series, oper: str ,values: Iterable[int],strict: bool=False) -> pd.DataFrame or pd.Series:\n",
    "    \"\"\"\n",
    "    Apply a comparison operation to a list of values and return all results that matches all elements in the list.\n",
    "\n",
    "    In strict mode, only keep rows that satisfy all matching requirements \n",
    "    \"\"\"\n",
    "    mult_result = (df_in != df_in) if oper == \"==\" else (df_in == df_in)\n",
    "    for val in ravel(values):\n",
    "        mult_result = {\n",
    "            \"<=\": mult_result & (df_in <= val), \"<\"  : mult_result & (df_in < val),\n",
    "            \">=\": mult_result & (df_in >= val), \">\"  : mult_result & (df_in > val),\n",
    "            \"==\": mult_result | (df_in == val), \"!=\" : mult_result & (df_in != val),\n",
    "        } [oper]\n",
    "    df_out = df_in[mult_result].dropna(how={True:\"any\",False:\"all\"}[strict])\n",
    "    return df_out\n",
    "\n",
    "def aggregate(df,identifiers,agg_columns=None,action=None,clip_outliers=None,**kwargs):    \n",
    "    \"\"\"\n",
    "    Aggregate a subset of columns in a DataFrame along a key column\n",
    "    \n",
    "    action == [`sum`,`mean`,`mode`,`median`,`count`]\n",
    "    \"\"\"\n",
    "    df_indexed = df.set_index(identifiers).sort_index()\n",
    "    unique_indexes = df_indexed.index.drop_duplicates()\n",
    "    agg_columns = isNone(agg_columns,then = df.columns.drop(identifiers))\n",
    "\n",
    "    df_aggregated = pd.DataFrame(index=unique_indexes,columns=agg_columns)\n",
    "    df_aggregated[\"action\"] = action\n",
    "    df_aggregated[\"count\"] = 0  \n",
    "\n",
    "    for index in unique_indexes:\n",
    "        df_group = df_indexed.loc[index,agg_columns]\n",
    "        if isinstance(df_group,pd.Series):\n",
    "            df_aggregated.loc[index,agg_columns] = df_group\n",
    "            df_aggregated.loc[index,\"count\"] = 1\n",
    "\n",
    "        # Clip outliers if specified\n",
    "        else:\n",
    "            if not isNone(clip_outliers):\n",
    "                df_group = remove_outliers(df_group,target_column=clip_outliers,**kwargs)\n",
    "            df_aggregated.loc[index,\"count\"] = len(df_group)\n",
    "            if action == \"mean\":\n",
    "                df_aggregated.loc[index,agg_columns] = df_group.mean().values\n",
    "            elif action == \"mode\":\n",
    "                df_aggregated.loc[index,agg_columns] = df_group.mode().values\n",
    "            elif action == \"median\":\n",
    "                df_aggregated.loc[index,agg_columns] = df_group.median().values\n",
    "            elif action == \"sum\":\n",
    "                df_aggregated.loc[index,agg_columns] = df_group.sum().values\n",
    "\n",
    "    df_aggregated = df_aggregated.reset_index()\n",
    "    return df_aggregated\n",
    "\n",
    "def groupings(df,identifiers,reset_index=True):\n",
    "    \"\"\"\n",
    "    Group the DataFrame into seperate DataFrame groupings based on the identifiers\n",
    "    \"\"\"\n",
    "    df_indexed = df.set_index(identifiers).sort_index()\n",
    "    unique_indexes = df_indexed.index.drop_duplicates()\n",
    "    if reset_index:\n",
    "        groups = [df_indexed.loc[idx,:].reset_index() for idx in unique_indexes]\n",
    "    else:\n",
    "        groups = [df_indexed.loc[idx,:] for idx in unique_indexes]\n",
    "    return groups\n",
    "\n",
    "\n",
    "def expand(series,index=None,columns=None):\n",
    "    \"\"\"\n",
    "    Expand a Series of n-sized arrays into a DataFrame with n columns\n",
    "    \"\"\"\n",
    "    if isinstance(series,pd.DataFrame):\n",
    "        series = series.iloc[:,-1]\n",
    "    if isinstance(series,pd.Series):\n",
    "        sr_len = len(series.iloc[0])\n",
    "        index = isNone(index, then = series.index)\n",
    "        columns = isNone(columns, then = [f\"{series.name}[{i}]\" for i in range(sr_len)])      \n",
    "\n",
    "    sr = apply(list(np.array(series)),list)\n",
    "    return pd.DataFrame(sr,index=index,columns=columns)\n",
    "\n",
    "\n",
    "def collapse(df,index=None,name=None):\n",
    "    \"\"\"\n",
    "    Collapse a DataFrame of n columns into a Series of n-sized arrays\n",
    "    \"\"\"\n",
    "    if isinstance(df,(pd.DataFrame,pd.Series)) and isNone(index):\n",
    "        index = df.index\n",
    "    df_ = apply(list(np.array(df)),np.array)\n",
    "    return pd.Series(df_,index=index,name=name)\n",
    "\n",
    "\n",
    "def remove_outliers(array,std_threshold=1,clip=\"both\",target_column=None):\n",
    "    \"\"\"\n",
    "    Remove outliers that are a certain standard deviation away from the mean. \n",
    "\n",
    "    Can clip either `low`, `high`, or `both` low and high outliers\n",
    "    \"\"\"\n",
    "    if isinstance(array,pd.DataFrame) and not isNone(target_column):\n",
    "        arr = array[target_column].values\n",
    "    else:\n",
    "        arr = np.array(array)\n",
    "\n",
    "    arr_mean = np.mean(arr)\n",
    "    arr_std = np.std(arr)\n",
    "    if arr_std > 0:\n",
    "        std_residuals = (arr - arr_mean)/arr_std\n",
    "    else:\n",
    "        std_residuals = np.zeros(shape=arr.shape)\n",
    "\n",
    "    ilocs = {\n",
    "        None : np.where(std_residuals == std_residuals)[0],\n",
    "        \"low\" : np.where(std_residuals >= -std_threshold)[0],\n",
    "        \"high\": np.where(std_residuals <= std_threshold)[0],\n",
    "        \"both\": np.where(np.abs(std_residuals) <= std_threshold)[0],\n",
    "    }[clip]\n",
    "\n",
    "    if isinstance(array,(pd.Series,pd.DataFrame)):\n",
    "        return array.iloc[ilocs]\n",
    "    else:\n",
    "        return array[ilocs]\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Encoders\n",
    "#----------------------------------------------\n",
    "\n",
    "class CategoricalEncoder:\n",
    "    def __init__(self,target_labels=None):\n",
    "        super().__init__()\n",
    "        self.target_labels = target_labels\n",
    "    \n",
    "    def fit(self,array):\n",
    "        array_labels = pd.unique(array)\n",
    "        self.target_labels = isNone(self.target_labels,then=range(len(array_labels)))\n",
    "        self.encode_mappings = dict(zip(array_labels,self.target_labels))\n",
    "        self.decode_mappings = dict(zip(self.target_labels,array_labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self,array,dtype=object):\n",
    "        return np.array([self.encode_mappings[a] for a in as_iterable(array)],dtype=dtype)\n",
    "\n",
    "    def inv_transform(self,array,dtype=object):\n",
    "        return np.array([self.decode_mappings[a] for a in as_iterable(array)],dtype=dtype)\n",
    "\n",
    "    def fit_transform(self,array,dtype=object):\n",
    "        return self.fit(array).transform(array,dtype)\n",
    "\n",
    "    def mappings(self):\n",
    "        return self.encode_mappings, self.decode_mappings\n",
    "\n",
    "\n",
    "class NullEncoder:\n",
    "    def __init__(self,fillna=None,unique=None):\n",
    "        self.fillna = fillna\n",
    "        self.unique = unique\n",
    "    \n",
    "    def fit(self,array):\n",
    "        arr = pd.Series(array)\n",
    "        if tryf(self.fillna,arr):\n",
    "            self.nan_value = self.fillna(arr)  \n",
    "        else:\n",
    "            self.nan_value = self.fillna\n",
    "        # Ensure that the nan value is unique (if required) by incrementing by a specified value\n",
    "        while self.nan_value in np.array(array) and self.unique:\n",
    "            self.nan_value += self.unique\n",
    "        return self\n",
    "\n",
    "    def transform(self,array):\n",
    "        return pd.Series(array).fillna(self.nan_value).values\n",
    "\n",
    "    def fit_transform(self,array):\n",
    "        return self.fit(array).transform(array)\n",
    "\n",
    "    def inv_transform(self,array):\n",
    "        return pd.Series(array).replace({self.nan_value: None}).values\n",
    "\n",
    "    def mappings(self):\n",
    "        return {None:self.nan_value},{self.nan_value:None}\n",
    "\n",
    "\n",
    "class AutoDataPrep:\n",
    "    def __init__(self,max_categories=10,\n",
    "                fillna_cont=np.mean, unique_cont=None,\n",
    "                fillna_cat=\"$null$\",unique_cat=None):\n",
    "        self.max_categories = max_categories\n",
    "        self.fillna_cont = fillna_cont\n",
    "        self.unique_cont = unique_cont\n",
    "        self.fillna_cat = fillna_cat\n",
    "        self.unique_cat = unique_cat\n",
    "        self.mappings_ = dict()\n",
    "\n",
    "    def fit_transform(self,X):\n",
    "        X_prep = X.copy()\n",
    "        for c in X.columns:\n",
    "            is_categorical = lambda x: dtype(x)==\"object\" or len(pd.unique(x)) <= self.max_categories # or not try_f(as_dtype(\"float\"),x)\n",
    "            equals = lambda a,b: np.all(a==b)\n",
    "            if is_categorical(X[c]):\n",
    "                encoders    = [ NullEncoder(fillna=self.fillna_cat,unique=self.unique_cat),\n",
    "                                CategoricalEncoder()]\n",
    "                X0 = encoders[0].fit_transform(X[c])\n",
    "                X1 = encoders[1].fit_transform(X0,dtype=int)\n",
    "                X_prep[c]   = X1\n",
    "                mappings = encoders[1].mappings()[0]\n",
    "                if self.fillna_cat in mappings:\n",
    "                    mappings.update({None:mappings.pop(self.fillna_cat)})\n",
    "            else:\n",
    "                encoders = [NullEncoder(fillna=self.fillna_cont,unique=self.unique_cont)]\n",
    "                X0 = encoders[0].fit_transform(X[c])\n",
    "                X_prep[c] = X0\n",
    "                if equals(X_prep[c],X[c]):\n",
    "                    mappings = None\n",
    "                else:\n",
    "                    mappings = encoders[0].mappings()[0]\n",
    "            self.mappings_[c] = mappings\n",
    "        return X_prep\n",
    "\n",
    "    def mappings(self):\n",
    "        return self.mappings_\n",
    "\n",
    "\n",
    "#----------------------------------------\n",
    "#   DATA TYPES\n",
    "#----------------------------------------\n",
    "\n",
    "def is_dtypes(df: pd.DataFrame or pd.Series ,dtypes : str or Iterable) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a series is of one or multiple numpy dtypes\n",
    "    - For `int` dtype, use `\"int64\"`\n",
    "    - For `float` dtype, use  `\"float64\"`\n",
    "    - For `object` dtype, use  `\"O\"`\n",
    "    - For `boolean` dtype, use `\"bool\"`\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(df, pd.Series):\n",
    "        return isinstance(df.dtypes,tuple([type(np.dtype(typ)) for typ in ravel(dtypes)]))\n",
    "    elif isinstance(df, Iterable):\n",
    "        return isinstance(np.array(df).dtype,tuple([type(np.dtype(typ)) for typ in ravel(dtypes)]))\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "        all_dtypes = True\n",
    "        for c in df.columns:\n",
    "            all_dtypes &= is_dtypes(df[c],dtypes) # Shallow recursion so should not affect performance\n",
    "        return all_dtypes\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_numeric(df: pd.DataFrame or pd.Series or Iterable) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a series is of numeric numpy dtypes\n",
    "    \"\"\"\n",
    "    return is_dtypes(df,(\"float64\",\"int64\"))\n",
    "\n",
    "\n",
    "def is_bool(df: pd.DataFrame or pd.Series or Iterable,binary_allowed: bool=False) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a DataFrame, Series or Iterable is of dtypes boolean.  \n",
    "    \"\"\"\n",
    "    # First check the dtype information about the object if available\n",
    "    is_dtype_bool = is_dtypes(df,\"bool\") \n",
    "\n",
    "    # In case the input dtype is not labeled as bool, or represented as a binary matrix instead\n",
    "    can_check_count = binary_allowed or not is_numeric(df)  \n",
    "    count_satisfied = False\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        count_satisfied = ((df == True)|(df == False)).sum().sum() == df.size\n",
    "    elif isinstance(df,pd.Series):\n",
    "        count_satisfied = ((df == True)|(df == False)).sum() == df.size\n",
    "    elif isinstance(df,Iterable):\n",
    "        arr = np.array(df,copy=True)\n",
    "        np.place(arr,(arr == True)|(arr == False),True)\n",
    "        count_satisfied = arr.sum() == len(arr)\n",
    "        \n",
    "    return is_dtype_bool or (can_check_count and count_satisfied) \n",
    "\n",
    "#----------------------------------------\n",
    "#   FEATURES\n",
    "#----------------------------------------\n",
    "\n",
    "\n",
    "def features_density(df,sort=False):\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        density = summary(df)[\"fill\"]\n",
    "    elif isinstance(df,pd.Series):\n",
    "        density = df.notna().replace({True:1,False:0})\n",
    "    if sort:\n",
    "        return density.sort_values(ascending=False)\n",
    "    else:\n",
    "        return density\n",
    "\n",
    "def features_density_by_threshold(df,threshold=None):\n",
    "    df_dens = features_density(df)\n",
    "    threshold = isNone(threshold,then=0)\n",
    "    if threshold > 1:\n",
    "        threshold = threshold/len(df)\n",
    "    return df_dens[df_dens >= threshold].sort_values(ascending=False)\n",
    "\n",
    "def features_density_by_rank(df,top=None):\n",
    "    df_dens = features_density(df)\n",
    "    top = isNone(top,then=len(df_dens))\n",
    "    if isinstance(top,float):\n",
    "        top = int(min(len(df_dens),len(df_dens)*top))\n",
    "    return df_dens.sort_values(ascending=False)[:top]\n",
    "\n",
    "def rows_density(df,sort=False):\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        density = as_binary_frame(df).sum(axis=1)/len(df.columns)\n",
    "    elif isinstance(df,pd.Series):\n",
    "        density = df.notna().replace({True:1,False:0})\n",
    "    if sort:\n",
    "        return density.sort_values(ascending=False)\n",
    "    else:\n",
    "        return density\n",
    "    # return as_binary_frame(df).sum(axis=1) / len(df.columns)\n",
    "\n",
    "def rows_density_by_threshold(df,threshold=None):\n",
    "    df_dens = rows_density(df)\n",
    "    threshold = isNone(threshold,then=0)\n",
    "    if threshold > 1:\n",
    "        threshold = threshold/len(df.columns)\n",
    "    return df_dens[df_dens >= threshold].sort_values(ascending=False)\n",
    "\n",
    "def rows_density_by_rank(df,top=None):\n",
    "    df_dens = rows_density(df)\n",
    "    top = isNone(top,then=len(df_dens))\n",
    "    if isinstance(top,float):\n",
    "        top = int(min(len(df_dens),len(df_dens)*top))\n",
    "    return df_dens.sort_values(ascending=False)[:top]\n",
    "\n",
    "def as_binary_frame(X,bool=False,nan_values=None):\n",
    "    X_bin = X.copy()\n",
    "    if not isNone(nan_values):\n",
    "        X_bin = match(X,\"!=\",nan_values)\n",
    "    if bool:\n",
    "        X_bin = X.notna()\n",
    "    else:\n",
    "        X_bin = X.notna().replace({True:1,False:0})\n",
    "    return X_bin\n",
    "\n",
    "def as_binary_matrix(X,bool=False,nan_values=None):\n",
    "    return as_binary_frame(X,bool,nan_values).values\n",
    "\n",
    "\n",
    "def randint_dataframe(low=2,high=None,size=None,seed=None):\n",
    "    np.random.seed(seed)\n",
    "    rd = np.random.randint(low,high,size)\n",
    "    return pd.DataFrame(rd)\n",
    "\n",
    "def random_dataframe(size=None,seed=None):\n",
    "    np.random.seed(seed)\n",
    "    rd = np.random.random(size)\n",
    "    return pd.DataFrame(rd)\n",
    "\n",
    "\n",
    "#----------------------------------------\n",
    "#   CLUSTERING\n",
    "#----------------------------------------\n",
    "\n",
    "def as_cluster_ilocs(cluster_labels):\n",
    "    cluster_ilocs = []\n",
    "    for i in np.sort(np.unique(cluster_labels)):\n",
    "        cluster_iloc = np.where(cluster_labels==i)[0]\n",
    "        cluster_ilocs.append(cluster_iloc)\n",
    "    return cluster_ilocs\n",
    "\n",
    "# def as_cluster_indexes(cluster_labels,index=None):\n",
    "#     cluster_idxes = []\n",
    "#     for i in np.sort(np.unique(cluster_labels)):\n",
    "#         cluster_iloc = np.where(cluster_labels==i)[0]\n",
    "#         if isNone(index):\n",
    "#             cluster_idxes.append(cluster_iloc)\n",
    "#         else:\n",
    "#             cluster_idxes.append(np.array(index)[cluster_iloc])\n",
    "#     return cluster_idxes\n",
    "\n",
    "def cluster_counts(cluster_ilocs_or_labels,y,drop_total=False):\n",
    "    \"\"\"\n",
    "    Find the class make-up of each cluster\n",
    "    \"\"\"\n",
    "    if isinstance(cluster_ilocs_or_labels[0],(pd.Int64Index,np.ndarray)): # if is_indexes\n",
    "        cluster_ilocs = cluster_ilocs_or_labels\n",
    "    else:\n",
    "        cluster_ilocs = as_cluster_ilocs(cluster_ilocs_or_labels)\n",
    "\n",
    "    classes, class_counts = label_counts(y).values(['labels','counts'])\n",
    "    # Calculate the class distribution for each cluster\n",
    "    results = pd.DataFrame()\n",
    "    for clust_idx_i in cluster_ilocs:\n",
    "        labels_i,counts_i,total_i = label_counts(y.iloc[clust_idx_i],\n",
    "                                                labels=classes).values(['labels','counts','total_count'])\n",
    "        cluser_i_cnt = PseudoObject(\n",
    "            **dict(zip(ravel(labels_i,str),counts_i)),\n",
    "            cluster_total = total_i\n",
    "        )\n",
    "        results = pd.concat([results,cluser_i_cnt.to_frame()],ignore_index=True)\n",
    "\n",
    "    # Add the class counts info\n",
    "    class_total = PseudoObject(\n",
    "        **dict(zip(ravel(classes,str),class_counts)),\n",
    "        cluster_total = np.sum(class_counts)\n",
    "    ).to_frame(index=[\"class_total\"])\n",
    "    results = pd.concat([results,class_total])\n",
    "\n",
    "    # Finally, format the result\n",
    "    results.index.name = \"cluster\"\n",
    "    if drop_total:\n",
    "        return results.drop(index=\"class_total\",columns=\"cluster_total\")\n",
    "    else:\n",
    "        return results\n",
    "\n",
    "def bimatrix_cluster(X,clusterer,**kwargs):\n",
    "    \"\"\"\n",
    "    Cluster the input based on features' (X) existence rather than their actual content.\n",
    "    \"\"\"\n",
    "    # Fit the clusterer to the binary matrix of X\n",
    "    clst = clusterer(**kwargs).fit(as_binary_matrix(X))\n",
    "\n",
    "    # Retrieve the indexes and counts from the clustering result\n",
    "    cluster_ilocs = as_cluster_ilocs(clst.labels_)\n",
    "    return cluster_ilocs #, cluster_cnts\n",
    "\n",
    "def load_clusters(cluster_ilocs,X=None,y=None,is_iloc=True):\n",
    "    results = []\n",
    "    if not isNone(X):\n",
    "        X_cl = [X.iloc[cl_i] for cl_i in cluster_ilocs]\n",
    "        results.append(X_cl)\n",
    "\n",
    "    if not isNone(y):\n",
    "        y_cl = [y.iloc[cl_i] for cl_i in cluster_ilocs]\n",
    "        results.append(y_cl)\n",
    "\n",
    "    if not isNone(X) and not isNone(y):\n",
    "        return results\n",
    "    else:\n",
    "        return results[0]\n",
    "\n",
    "def partition_clusters(cluster_ilocs,X,y,rows_density=None,features_density=None):\n",
    "    partitions = []\n",
    "    X_cl,y_cl = load_clusters(cluster_ilocs,X,y)\n",
    "    for X_i,y_i in zip(X_cl,y_cl):\n",
    "        index = rows_density_by_threshold(X_i,rows_density).index\n",
    "        columns = features_density_by_threshold(X_i,features_density).index\n",
    "        partitions.append([X_i.loc[index,columns],y_i.loc[index]])\n",
    "    return partitions\n",
    "\n",
    "def features_density_summary(df):\n",
    "    feat_dens_sum = pd.Series(dtype=float)\n",
    "    for t in [0.25,0.5,0.75]:\n",
    "        feat_dens_sum.loc[f\"d(top {int(t*100)}%)\"] = features_density_by_rank(df,top=t).mean()\n",
    "    for t in [0.25,0.5,0.75]:\n",
    "        feat_dens_sum.loc[f\"n(top {int(t*100)}%)\"] = len(features_density_by_rank(df,top=t))\n",
    "    for d in [0.75,0.5,0.25]:\n",
    "        feat_dens_sum.loc[f\"d(d > {d})\"] = features_density_by_threshold(df,d).mean()\n",
    "    for d in [0.75,0.5,0.25]:\n",
    "        feat_dens_sum.loc[f\"n(d > {d})\"] = len(features_density_by_threshold(df,d))\n",
    "    feat_dens_sum.loc[f\"overall\"] = features_density(df).mean()\n",
    "    return feat_dens_sum\n",
    "\n",
    "# All our data \n",
    "def cluster_mse_table(cluster_ilocs,X,y):\n",
    "    \"\"\"\n",
    "    Table of SSE for each cluster\n",
    "    \"\"\"\n",
    "    X_cl = load_clusters(cluster_ilocs,X=X)\n",
    "    sse_tabl = pd.DataFrame()\n",
    "    for X_i in X_cl:\n",
    "        X_bin = as_binary_frame(X)\n",
    "        X_i_mean = features_density(X_i) # Our average as the ground truth\n",
    "        sse_i = ((X_bin - X_i_mean)**2).mean(axis=1) # Calculate the SSE for all data\n",
    "        sse_tabl = pd.concat([sse_tabl,sse_i],axis=1,ignore_index=True)\n",
    "    sse_tabl['y'] = y\n",
    "\n",
    "    sse_tabl.index.name = 'index'\n",
    "    sse_tabl.columns.name = 'cluster'\n",
    "    return sse_tabl\n",
    "\n",
    "def cluster_likeness_table(cluster_ilocs,X,y,average=True,TN=False):\n",
    "    \"\"\"\n",
    "    Table for total likeness of each row with a cluster. The bigger the value the better\n",
    "\n",
    "    `average`: the likeness is averaged over each cluster\n",
    "    \"\"\"\n",
    "    feat_in_common_tbl = feat_in_common_frame(X,normalized=True,TN=TN)\n",
    "    cl_likeness_tbl = pd.DataFrame()\n",
    "    for cl_i in cluster_ilocs:\n",
    "        if average:\n",
    "            cl_i_likeness_res = feat_in_common_tbl.loc[:,cl_i].mean(axis=1)\n",
    "        else:\n",
    "            cl_i_likeness_res = feat_in_common_tbl.loc[:,cl_i].sum(axis=1)\n",
    "        cl_likeness_tbl = pd.concat([cl_likeness_tbl,cl_i_likeness_res],axis=1,ignore_index=True)\n",
    "    cl_likeness_tbl['y'] = y\n",
    "\n",
    "    cl_likeness_tbl.index.name = 'index'\n",
    "    cl_likeness_tbl.columns.name = 'cluster'\n",
    "\n",
    "    return cl_likeness_tbl\n",
    "\n",
    "\n",
    "def cluster_reindex(cluster_ilocs,X,y,quota=10,criteria=cluster_mse_table,ascending=True,**kwargs):\n",
    "    crit_table = criteria(cluster_ilocs,X,y,**kwargs)\n",
    "    classes,num_classes = label_counts(y).values(['labels','num_classes'])\n",
    "\n",
    "    cluster_ilocs_new = []\n",
    "    for i in range(len(cluster_ilocs)):\n",
    "        cl_i = cluster_ilocs[i]\n",
    "        y_i = y.iloc[cl_i]\n",
    "        class_counts_i = label_counts(y_i,labels=classes).counts\n",
    "\n",
    "        cl_ilocs_new = pd.Index(cl_i)\n",
    "        for j in classes:\n",
    "            index1 = y.index.drop(y_i.index)    # Set of all cases not in this cluster\n",
    "            index2 = crit_table[crit_table['y'] == j].index   # Set of cases that is of class j\n",
    "            diff_quota = max(0,quota-class_counts_i[j])\n",
    "            crit_table_ij = crit_table.loc[overlap(index1,index2),i].sort_values(ascending=ascending)[:diff_quota]\n",
    "            cl_ilocs_new = cl_ilocs_new.append(crit_table_ij.index)\n",
    "        cluster_ilocs_new.append(cl_ilocs_new.values)\n",
    "    return cluster_ilocs_new\n",
    "\n",
    "def cluster_cofreq_matrix(cluster_ilocs_or_labels):\n",
    "    \"\"\"\n",
    "    Adjacency matrix indicating the frequency with which each pair of elements is grouped into the same cluster.\n",
    "    \"\"\"\n",
    "    if isinstance(cluster_ilocs_or_labels[0],(pd.Int64Index,np.ndarray)): # if is_indexes\n",
    "        cluster_ilocs = cluster_ilocs_or_labels\n",
    "    else:\n",
    "        cluster_ilocs = as_cluster_ilocs(cluster_ilocs_or_labels)\n",
    "\n",
    "    # dim_size = np.sum([len(cl_i) for cl_i in cluster_ilocs])\n",
    "    # dim_size = label_counts(cluster_ilocs).num_classes\n",
    "    \n",
    "    dim_size = max(ravel(label_counts(cluster_ilocs).labels))+1\n",
    "    sim_matrix = np.zeros(shape=(dim_size,dim_size))\n",
    "    for cl_i in cluster_ilocs:\n",
    "        for j in range(len(cl_i)):\n",
    "            idx_ij = cl_i[j]\n",
    "            for k in range(j,len(cl_i)):\n",
    "                idx_ik = cl_i[k]\n",
    "                sim_matrix[idx_ij,idx_ik] += 1\n",
    "                if idx_ij != idx_ik: # Avoid adding the same cell twice\n",
    "                    sim_matrix[idx_ik,idx_ij] += 1\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "def bimatrix_cluster_cv(X,cluster_args,clusterer_cv,return_cofreq_matrix=False,**kwargs):\n",
    "    cofreq_matrix_cv = np.zeros(shape=(len(X),len(X)))\n",
    "    for clusterer,args in cluster_args:\n",
    "        cluster_ilocs_i = bimatrix_cluster(X,clusterer,**args)\n",
    "        cofreq_matrix_i = cluster_cofreq_matrix(cluster_ilocs_i)\n",
    "        cofreq_matrix_cv += cofreq_matrix_i\n",
    "\n",
    "    # Similar to bimatrix_cluster but now we fit the sim_matrix using the clusterer_cv\n",
    "    clst = clusterer_cv(**kwargs).fit(cofreq_matrix_cv)\n",
    "    cluster_ilocs_cv = as_cluster_ilocs(clst.labels_)\n",
    "    if return_cofreq_matrix:\n",
    "        return cluster_ilocs_cv,cofreq_matrix_cv\n",
    "    else:\n",
    "        return cluster_ilocs_cv\n",
    "\n",
    "def feat_in_common_matrix(df,normalized=True,TN=False):\n",
    "    \"\"\"\n",
    "    When TN = `False`, only calculate the TP counts of column matches between 2 elements. Else calculate both the TP and TN \n",
    "\n",
    "    When normalized = `True`: calculate the matches averaged over the number of features \n",
    "    \"\"\"\n",
    "    df_bin = as_binary_matrix(df)\n",
    "    nrows,ncols = df_bin.shape\n",
    "    feat_in_common_mat = []\n",
    "    for row_i in df_bin:\n",
    "        if not TN:\n",
    "            feat_in_common_mat.append(((df_bin==row_i)&(row_i==1)).sum(axis=1))\n",
    "        else:\n",
    "            feat_in_common_mat.append((df_bin==row_i).sum(axis=1))\n",
    "    if normalized:\n",
    "        return np.array(feat_in_common_mat) / ncols\n",
    "    else:\n",
    "        return np.array(feat_in_common_mat)\n",
    "\n",
    "def feat_in_common_frame(df,normalized=True,TN=False,sum=False):\n",
    "    df = pd.DataFrame(feat_in_common_matrix(df,normalized,TN))\n",
    "    if sum:\n",
    "        df['sum'] = df.sum(axis=1)\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
