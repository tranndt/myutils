{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.kernel_ridge import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"../../\")\n",
    "from myutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_scores(y_true,y_pred,pos_label=1,average=\"auto\",catch=False):\n",
    "    # Calculating the scores\n",
    "    f = lambda *args,**kwargs: tryf_catch(*args,**kwargs) if catch else tryf_return(np.nan,*args,**kwargs)\n",
    "    true_counts,true_labels,true_n_class = f(label_counts(y_true).values,['counts','labels','num_classes'])\n",
    "    pred_counts = f(lambda x: label_counts(x,labels=true_labels).counts,y_pred) # Coerce the counts to be in the same order as the true counts\n",
    "    mcm = f(multilabel_confusion_matrix,y_true,y_pred,labels=true_labels)\n",
    "    corr_counts = f(lambda m: np.array([m[i,-1,-1] for i in range(true_n_class)]), mcm)\n",
    "    # tpr = f(lambda corr,true: corr/true,corr_counts,true_counts)\n",
    "\n",
    "    if true_n_class == 2:\n",
    "        average = \"binary\" if average == \"auto\" else average\n",
    "        conf_matrix = f(lambda yt,yp: confusion_matrix(yt,yp,labels=true_labels).ravel(),y_true,y_pred)\n",
    "    else:\n",
    "        average = \"macro\" if average == \"auto\" else average\n",
    "        conf_matrix = f(lambda yt,yp: confusion_matrix(yt,yp,labels=true_labels),y_true,y_pred)\n",
    "\n",
    "    precision = f(precision_score,y_true,y_pred,pos_label=pos_label,average=average,zero_division=0)\n",
    "    recall = f(recall_score,y_true,y_pred,pos_label=pos_label,average=average,zero_division=0)\n",
    "    accuracy = f(accuracy_score,y_true,y_pred)\n",
    "    balanced_accuracy = f(balanced_accuracy_score,y_true,y_pred)\n",
    "    roc_auc = f(roc_auc_score,y_true,y_pred)\n",
    "    f1_binary = f(f1_score,y_true,y_pred,pos_label=pos_label,average=\"binary\")\n",
    "    f1_micro =  f(f1_score,y_true,y_pred,pos_label=pos_label,average=\"micro\")\n",
    "    f1_macro =  f(f1_score,y_true,y_pred,pos_label=pos_label,average=\"macro\")\n",
    "    f1_weighted = f(f1_score,y_true,y_pred,pos_label=pos_label,average=\"weighted\")\n",
    "\n",
    "    # Save the score\n",
    "    scores = dict()\n",
    "    scores['test_labels'] = true_labels\n",
    "    scores['test_counts'] = true_counts\n",
    "    scores['pred_counts'] = pred_counts\n",
    "    scores['corr_counts'] = corr_counts\n",
    "    scores['confusion_matrix'] = conf_matrix\n",
    "    scores['f1_binary'] = f1_binary\n",
    "    scores['f1_micro'] = f1_micro\n",
    "    scores['f1_macro'] = f1_macro\n",
    "    scores['f1_weighted'] = f1_weighted\n",
    "    for l,i in zip(true_labels,range(true_n_class)):\n",
    "        scores[f'tpr[{l}]'] = f(lambda corr,true,x: (corr/true)[x],corr_counts,true_counts,i)\n",
    "    scores['accuracy'] = accuracy\n",
    "    scores['balanced_accuracy'] = balanced_accuracy\n",
    "    scores[f'precision_{average}'] = precision\n",
    "    scores[f'recall_{average}'] = recall\n",
    "    scores['auc'] = roc_auc\n",
    "    return scores\n",
    "\n",
    "def regression_scores(y_true,y_pred):\n",
    "    test_counts = len(y_true)\n",
    "    test_quartiles = np.round(np.quantile(y_true,[0,0.25,0.5,0.75,1]),2)\n",
    "    pred_quartiles = np.round(np.quantile(y_pred,[0,0.25,0.5,0.75,1]),2)\n",
    "\n",
    "    spread_counts = lambda y_p: np.array([len(y_p[y_p < y_true.min()]),*np.histogram(y_p,test_quartiles)[0],len(y_p[y_p > y_true.max()])])\n",
    "    test_spread = spread_counts(y_true)\n",
    "    pred_spread = spread_counts(y_pred)\n",
    "\n",
    "    explained_variance_ = tryf_return (None,explained_variance_score,y_true,y_pred)\n",
    "    max_error_ = tryf_return (None,max_error,y_true,y_pred)\n",
    "    neg_mean_absolute_error_ = tryf_return (None,mean_absolute_error,y_true,y_pred)\n",
    "    neg_mean_squared_error_ = tryf_return (None,mean_squared_error,y_true,y_pred,squared=True)\n",
    "    neg_root_mean_squared_error_ = tryf_return (None,mean_squared_error,y_true,y_pred,squared=False)\n",
    "    neg_mean_squared_log_error_ = tryf_return (None,mean_squared_log_error,y_true,y_pred)\n",
    "    neg_median_absolute_error_ = tryf_return (None,median_absolute_error,y_true,y_pred) \n",
    "    r2_ = tryf_return (None,r2_score,y_true,y_pred)\n",
    "    neg_mean_poisson_deviance_ = tryf_return (None,mean_poisson_deviance,y_true,y_pred)\n",
    "    neg_mean_gamma_deviance_ = tryf_return (None,mean_gamma_deviance,y_true,y_pred)\n",
    "    neg_mean_absolute_percentage_error_ = tryf_return (None,mean_absolute_percentage_error,y_true,y_pred)\n",
    "\n",
    "    scores = dict()\n",
    "    scores['test_counts'] = test_counts\n",
    "    scores['test_quartiles'] = test_quartiles\n",
    "    scores['pred_quartiles'] = pred_quartiles\n",
    "    scores['test_spread'] = test_spread\n",
    "    scores['pred_spread'] = pred_spread\n",
    "    scores['R2'] = r2_\n",
    "    scores['explained_variance'] = explained_variance_\n",
    "    scores['max_error'] = max_error_\n",
    "    scores['MAE']   = neg_mean_absolute_error_\n",
    "    scores['MedAE'] = neg_median_absolute_error_\n",
    "    scores['MSE']   = neg_mean_squared_error_\n",
    "    scores['RMSE']  = neg_root_mean_squared_error_\n",
    "    scores['MSLE']  = neg_mean_squared_log_error_\n",
    "    scores['MAPE']  = neg_mean_absolute_percentage_error_\n",
    "    scores['mean_poisson_deviance'] = neg_mean_poisson_deviance_\n",
    "    scores['mean_gamma_deviance']   = neg_mean_gamma_deviance_\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_labels</th>\n",
       "      <th>test_counts</th>\n",
       "      <th>pred_counts</th>\n",
       "      <th>corr_counts</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>f1_binary</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tpr[1]</th>\n",
       "      <th>tpr[2]</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_binary</th>\n",
       "      <th>recall_binary</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>[12, 17]</td>\n",
       "      <td>[12, 11, 10, 17]</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.579152</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.575684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_labels test_counts pred_counts corr_counts  confusion_matrix  \\\n",
       "0      [1, 2]    [23, 27]    [22, 28]    [12, 17]  [12, 11, 10, 17]   \n",
       "\n",
       "   f1_binary  f1_micro  f1_macro  f1_weighted    tpr[1]   tpr[2]  accuracy  \\\n",
       "0   0.533333      0.58  0.575758     0.579152  0.521739  0.62963      0.58   \n",
       "\n",
       "   balanced_accuracy  precision_binary  recall_binary       auc  \n",
       "0           0.575684          0.545455       0.521739  0.575684  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "y_true = np.random.randint(1,3,50)\n",
    "y_pred = np.random.randint(1,3,50)\n",
    "pd.DataFrame([classification_scores(y_true,y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_counts</th>\n",
       "      <th>test_quartiles</th>\n",
       "      <th>pred_quartiles</th>\n",
       "      <th>test_spread</th>\n",
       "      <th>pred_spread</th>\n",
       "      <th>R2</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>max_error</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MedAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSLE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>mean_poisson_deviance</th>\n",
       "      <th>mean_gamma_deviance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>[0.0, 0.19, 0.42, 0.74, 0.99]</td>\n",
       "      <td>[-0.5, -0.23, 0.05, 0.2, 0.46]</td>\n",
       "      <td>[0, 13, 13, 11, 13, 0]</td>\n",
       "      <td>[23, 13, 10, 4, 0, 0]</td>\n",
       "      <td>-1.329747</td>\n",
       "      <td>0.962478</td>\n",
       "      <td>0.561547</td>\n",
       "      <td>0.462195</td>\n",
       "      <td>0.476233</td>\n",
       "      <td>0.217121</td>\n",
       "      <td>0.465963</td>\n",
       "      <td>None</td>\n",
       "      <td>89.456522</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_counts                 test_quartiles                  pred_quartiles  \\\n",
       "0           50  [0.0, 0.19, 0.42, 0.74, 0.99]  [-0.5, -0.23, 0.05, 0.2, 0.46]   \n",
       "\n",
       "              test_spread            pred_spread        R2  \\\n",
       "0  [0, 13, 13, 11, 13, 0]  [23, 13, 10, 4, 0, 0] -1.329747   \n",
       "\n",
       "   explained_variance  max_error       MAE     MedAE       MSE      RMSE  \\\n",
       "0            0.962478   0.561547  0.462195  0.476233  0.217121  0.465963   \n",
       "\n",
       "   MSLE       MAPE mean_poisson_deviance mean_gamma_deviance  \n",
       "0  None  89.456522                  None                None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "y_true = np.sort(np.random.random(50))\n",
    "y_pred = np.sort(np.random.random(50)) - 0.5\n",
    "pd.DataFrame([regression_scores(y_true,y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import *\n",
    "import warnings\n",
    "\n",
    "class AutoClassifier:    \n",
    "    def __init__(self,models=None,verbose=True,ignore_warnings=False):\n",
    "        \"\"\"\n",
    "        - `models`: \n",
    "            - `\"all\"`: All models\n",
    "            - `\"auto\"`: Common classification models\n",
    "                - LogisticRegression([penalty, ...]) Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "                - SGDClassifier([loss, penalty, ...]) Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
    "                - SVC(*[, C, kernel, degree, gamma, ...]) C-Support Vector Classification.\n",
    "                - DecisionTreeClassifier(*[, criterion, ...]) A decision tree classifier.\n",
    "                - GradientBoostingClassifier(*[, ...]) Gradient Boosting for classification.\n",
    "                - RandomForestClassifier([...]) A random forest classifier.\n",
    "                - GaussianProcessClassifier([...]) Gaussian process classification (GPC) based on Laplace approximation.\n",
    "                - GaussianNB(*[, priors, ...]) Gaussian Naive Bayes (GaussianNB).\n",
    "                - MLPClassifier -- Multi-layer Perceptron classifier.\n",
    "\n",
    "            - `\"linear\"`: \n",
    "                - LogisticRegression([penalty, ...]) Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "                - LogisticRegressionCV(*[, Cs, ...]) Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
    "                - PassiveAggressiveClassifier(*) Passive Aggressive Classifier.\n",
    "                - Perceptron(*[, penalty, alpha, ...]) Linear perceptron classifier.\n",
    "                - RidgeClassifier([alpha, ...]) Classifier using Ridge regression.\n",
    "                - RidgeClassifierCV([alphas, ...]) Ridge classifier with built-in cross-validation.\n",
    "                - SGDClassifier([loss, penalty, ...]) Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
    "                - SGDOneClassSVM([nu, ...]) Solves linear One-Class SVM using Stochastic Gradient Descent.\n",
    "\n",
    "            - `\"svm\"`:\n",
    "                - LinearSVC([penalty, loss, dual, tol, C, ...]) Linear Support Vector Classification.\n",
    "                - NuSVC(*[, nu, kernel, degree, gamma, ...]) Nu-Support Vector Classification.\n",
    "                - SVC(*[, C, kernel, degree, gamma, ...]) C-Support Vector Classification.\n",
    "\n",
    "            - `\"neighbors\"` \n",
    "                - KNeighborsClassifier([...]) Classifier implementing the k-nearest neighbors vote.\n",
    "\n",
    "            - `\"tree\"`\n",
    "                - DecisionTreeClassifier(*[, criterion, ...]) A decision tree classifier.\n",
    "                - ExtraTreeClassifier(*[, criterion, ...]) An extremely randomized tree classifier.\n",
    "\n",
    "            - `\"ensemble\"`\n",
    "                - AdaBoostClassifier([...]) An AdaBoost classifier.\n",
    "                - ExtraTreesClassifier([...]) An extra-trees classifier.\n",
    "                - GradientBoostingClassifier(*[, ...]) Gradient Boosting for classification.\n",
    "                - RandomForestClassifier([...]) A random forest classifier.\n",
    "\n",
    "            - `\"gaussian_process\"`\n",
    "                - GaussianProcessClassifier([...]) Gaussian process classification (GPC) based on Laplace approximation.\n",
    "\n",
    "            - `\"naive_bayes\"`\n",
    "                - BernoulliNB(*[, alpha, ...]) Naive Bayes classifier for multivariate Bernoulli models.\n",
    "                - ComplementNB(*[, alpha, ...]) The Complement Naive Bayes classifier described in Rennie et al. (2003).\n",
    "                - GaussianNB(*[, priors, ...]) Gaussian Naive Bayes (GaussianNB).\n",
    "                - MultinomialNB(*[, alpha, ...]) Naive Bayes classifier for multinomial models.\n",
    "\n",
    "            - `\"neural_network\"`\n",
    "                - MLPClassifier -- Multi-layer Perceptron classifier.\n",
    "\n",
    "            - Reference:\n",
    "                - [sklearn.neighbors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n",
    "                - [sklearn.gaussian_process](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.gaussian_process)\n",
    "                - [sklearn.neural_network](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network)\n",
    "                - [sklearn.svm](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "                - [sklearn.tree](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n",
    "                - [sklearn.linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "                - [sklearn.naive_bayes](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n",
    "                - [sklearn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "        \"\"\"\n",
    "        self.DEFAULT_MODELS = {\n",
    "        #tree\n",
    "            'DecisionTreeClassifier': ('tree',DecisionTreeClassifier()),\n",
    "            'ExtraTreeClassifier': ('tree',ExtraTreeClassifier()),\n",
    "            # linear\n",
    "            'LogisticRegression': ('linear',LogisticRegression()),\n",
    "            'LogisticRegressionCV': ('linear',LogisticRegressionCV()),\n",
    "            'PassiveAggressiveClassifier': ('linear',PassiveAggressiveClassifier()),\n",
    "            'Perceptron': ('linear',Perceptron()),\n",
    "            'RidgeClassifier': ('linear',RidgeClassifier()),\n",
    "            'RidgeClassifierCV': ('linear',RidgeClassifierCV()),\n",
    "            'SGDClassifier': ('linear',SGDClassifier()),\n",
    "            # svm\n",
    "            'LinearSVC': ('svm',LinearSVC()),\n",
    "            'NuSVC': ('svm',NuSVC()),\n",
    "            'SVC': ('svm',SVC()),\n",
    "            # neighbors\n",
    "            'KNeighborsClassifier': ('neighbors',KNeighborsClassifier()),\n",
    "            # ensemble\n",
    "            'AdaBoostClassifier': ('ensemble',AdaBoostClassifier()),\n",
    "            'ExtraTreesClassifier': ('ensemble',ExtraTreesClassifier()),\n",
    "            'GradientBoostingClassifier': ('ensemble',GradientBoostingClassifier()),\n",
    "            'RandomForestClassifier': ('ensemble',RandomForestClassifier()),\n",
    "            # gaussian_process\n",
    "            # 'GaussianProcessClassifier': ('gaussian_process',GaussianProcessClassifier()),\n",
    "            # naive_bayes\n",
    "            'BernoulliNB': ('naive_bayes',BernoulliNB()),\n",
    "            'ComplementNB': ('naive_bayes',ComplementNB()),\n",
    "            'GaussianNB': ('naive_bayes',GaussianNB()),\n",
    "            'MultinomialNB': ('naive_bayes',MultinomialNB()),\n",
    "            # neural_network\n",
    "            'MLPClassifier': ('neural_network',MLPClassifier())\n",
    "        }\n",
    "        self.verbose = verbose\n",
    "        self.__init_models__(models)\n",
    "        self.__filter_warnings__(ignore_warnings)\n",
    "        pass\n",
    "\n",
    "    def __filter_warnings__(self,ignore_warnings):\n",
    "        if ignore_warnings is not False:\n",
    "            if ignore_warnings is True:\n",
    "                warnings_ = [\n",
    "                    ConvergenceWarning,\n",
    "                    DataConversionWarning,\n",
    "                    DataDimensionalityWarning,\n",
    "                    EfficiencyWarning,\n",
    "                    UndefinedMetricWarning,\n",
    "                    RuntimeWarning,\n",
    "                    FutureWarning,\n",
    "                    np.VisibleDeprecationWarning\n",
    "                ]\n",
    "            for w in as_iterable(warnings_):\n",
    "                tryf_return (None,warnings.filterwarnings,\"ignore\", category=w)\n",
    "\n",
    "    def __init_models__(self, models):\n",
    "        get_models_         = lambda x: at(list(self.DEFAULT_MODELS.keys()),whether(self.DEFAULT_MODELS.values(),lambda v: v[0] == x))\n",
    "        get_group          = lambda x: tryf_return(\"other\", lambda m: self.DEFAULT_MODELS[m][0],x)\n",
    "        is_module_wo_name   = lambda x: type_of(x).startswith(\"ABCMeta\")\n",
    "        is_module_w_name    = lambda x: isinstance(x,Iterable) and type_of(x[1]).startswith(\"ABCMeta\")\n",
    "        is_model_name_key   = lambda x: isinstance(x,str) and x in self.DEFAULT_MODELS.keys()\n",
    "        is_group_key       = lambda x: isinstance(x,str) and x in [v[0] for v in self.DEFAULT_MODELS.values()]\n",
    "\n",
    "        models_ = []\n",
    "        if isNone(models):\n",
    "            models_ = list(self.DEFAULT_MODELS.values())\n",
    "            \n",
    "        else:\n",
    "            for option in as_iterable(models):\n",
    "                if is_group_key(option):\n",
    "                    for m in get_models_(option):\n",
    "                        models += self.DEFAULT_MODELS[m]\n",
    "\n",
    "                elif is_model_name_key(option):\n",
    "                    models += self.DEFAULT_MODELS[option]\n",
    "\n",
    "                elif is_module_wo_name(option):\n",
    "                    models.append((get_group(option),option))\n",
    "\n",
    "                elif is_module_w_name(option):\n",
    "                    models.append(option)\n",
    "\n",
    "        self.models = models_\n",
    "\n",
    "\n",
    "    def trained_models(self):\n",
    "        return [m for n,m in self.models]\n",
    "\n",
    "    def model_names(self):\n",
    "        return [type_of(m) for n,m in self.models]\n",
    "\n",
    "    def model_groups(self):\n",
    "        return [n for n,m in self.models]\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        for m in self.trained_models():\n",
    "            flushif(self.verbose,f\"\\n> Fitting {type_of(m)}\")\n",
    "            t = TaskTimer()\n",
    "            total_time, m = t.execute(\n",
    "                tryf_return,m,m.fit,X,y\n",
    "            )\n",
    "            flushif(self.verbose,f\"\\r>> Fitted {type_of(m)} in {t.fmt(total_time)}\")\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        predictions = []\n",
    "        for m in self.trained_models():\n",
    "            predictions_i = tryf_catch(m.predict,X)\n",
    "            predictions.append(predictions_i)\n",
    "        predictions = np.array(predictions,dtype=object)\n",
    "        return predictions\n",
    "\n",
    "    def fit_predict(self,X,y,X_test):\n",
    "        return self.fit(X,y).predict(X_test)\n",
    "        \n",
    "    def score(self,X,y,catch=False):\n",
    "        y_pred = self.predict(X)\n",
    "        scores = pd.DataFrame()\n",
    "        for (n,m),y_i in zip(self.models,y_pred):\n",
    "            iter_score = classification_scores(y,y_i,catch=catch)\n",
    "            iter_score[\"model\"] = type_of(m)\n",
    "            iter_score[\"params\"] = tryf_return(str({}),str(m.get_params))\n",
    "            iter_score[\"group\"] = n\n",
    "            scores = pd.concat([scores,pd.DataFrame([iter_score])],ignore_index=True)\n",
    "        scores = scores.set_index([\"model\",\"params\",\"group\"])\n",
    "        return scores.reset_index([\"model\",\"params\",\"group\"])\n",
    "\n",
    "    def cross_validate(self,X,y,cv=3,catch=False):\n",
    "        if isinstance(cv,int):\n",
    "            cv = range(cv)\n",
    "        scores = pd.DataFrame()\n",
    "        for i in as_iterable(cv):\n",
    "            if tryf(train_test_split,X,y,stratify=y,random_state=i):\n",
    "                X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,random_state=i)\n",
    "            else:\n",
    "                X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=i)\n",
    "            self.fit(X_train,y_train)\n",
    "            scores_i = self.score(X_test,y_test,catch)\n",
    "            scores_i[\"seed\"] = i\n",
    "            scores = pd.concat([scores,scores_i],axis=0,ignore_index=True)\n",
    "        return scores\n",
    "\n",
    "class AutoRegressor(AutoClassifier):\n",
    "    def __init__(self,models=None,verbose=True,ignore_warnings=False):\n",
    "        \"\"\"\n",
    "        - `models`\n",
    "            - `svm`:\n",
    "                - LinearSVR(*[, epsilon, tol, C, loss, ...]) Linear Support Vector Regression.\n",
    "                - NuSVR(*[, nu, C, kernel, degree, gamma, ...]) Nu Support Vector Regression.\n",
    "                - SVR(*[, kernel, degree, gamma, coef0, ...]) Epsilon-Support Vector Regression.\n",
    "\n",
    "            - `neighbors` \n",
    "                - KNeighborsRegressor([n_neighbors, ...]) Regression based on k-nearest neighbors.\n",
    "\n",
    "            - `tree`\n",
    "                - DecisionTreeRegressor(*[, criterion, ...]) A decision tree regressor.\n",
    "                - ExtraTreeRegressor(*[, criterion, ...]) An extremely randomized tree regressor.  \n",
    "\n",
    "            - `ensemble`: Combine the predictions of several base estimators in order to improve generalizability / robustness. -- In averaging methods, reduce variance by building several estimators independently and then averaging their predictions (Bagging methods, Forests of randomized trees, …) -- In boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. (AdaBoost, Gradient Tree Boosting, …)\n",
    "                - AdaBoostRegressor([base_estimator, ...]) An AdaBoost regressor.\n",
    "                - BaggingRegressor([base_estimator, ...]) A Bagging regressor.\n",
    "                - ExtraTreesRegressor([n_estimators, ...]) An extra-trees regressor.\n",
    "                - GradientBoostingRegressor(*[, ...]) Gradient Boosting for regression.\n",
    "                - RandomForestRegressor([...]) A random forest regressor.\n",
    "                - StackingRegressor(estimators[, ...]) Stack of estimators with a final regressor.\n",
    "                - VotingRegressor(estimators, *[, ...]) Prediction voting regressor for unfitted estimators.\n",
    "                - HistGradientBoostingRegressor([...]) Histogram-based Gradient Boosting Regression Tree.\n",
    "\n",
    "            - `gaussian_process`\n",
    "                - GaussianProcessRegressor([...]) Gaussian process regression (GPR).\n",
    "\n",
    "            - `neural_network`\n",
    "                - MLPRegressor -- Multi-layer Perceptron regressor.\n",
    "\n",
    "            - `linear`: Classical linear regressors\n",
    "                - LinearRegression(*[, ...]) Ordinary least squares Linear Regression.\n",
    "                - Ridge([alpha, fit_intercept, ...]) Linear least squares with l2 regularization.\n",
    "                - RidgeCV([alphas, ...]) Ridge regression with built-in cross-validation.\n",
    "                - SGDRegressor([loss, penalty, ...]) Linear model fitted by minimizing a regularized empirical loss with SGD.\n",
    "\n",
    "            - `linear-variable-selection`: (Linear) Regressors with variable selection. The following estimators have built-in variable selection fitting procedures, but any estimator using a L1 or elastic-net penalty also performs variable selection: typically SGDRegressor or SGDClassifier with an appropriate penalty.\n",
    "                - ElasticNet([alpha, l1_ratio, ...]) Linear regression with combined L1 and L2 priors as regularizer.\n",
    "                - ElasticNetCV(*[, l1_ratio, ...]) Elastic Net model with iterative fitting along a regularization path.\n",
    "                - Lars(*[, fit_intercept, ...]) Least Angle Regression model a.k.a.\n",
    "                - LarsCV(*[, fit_intercept, ...]) Cross-validated Least Angle Regression model.\n",
    "                - Lasso([alpha, fit_intercept, ...]) Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
    "                - LassoCV(*[, eps, n_alphas, ...]) Lasso linear model with iterative fitting along a regularization path.\n",
    "                - LassoLars([alpha, ...]) Lasso model fit with Least Angle Regression a.k.a.\n",
    "                - LassoLarsCV(*[, fit_intercept, ...]) Cross-validated Lasso, using the LARS algorithm.\n",
    "                - LassoLarsIC([criterion, ...]) Lasso model fit with Lars using BIC or AIC for model selection.\n",
    "                - OrthogonalMatchingPursuit(*[, ...]) Orthogonal Matching Pursuit model (OMP).\n",
    "                - OrthogonalMatchingPursuitCV(*) Cross-validated Orthogonal Matching Pursuit model (OMP).\n",
    "\n",
    "            - `bayesian`: (Linear) Bayesian regressors\n",
    "                - ARDRegression(*[, n_iter, tol, ...]) Bayesian ARD regression.\n",
    "                - BayesianRidge(*[, n_iter, tol, ...]) Bayesian ridge regression.\n",
    "\n",
    "            - `multitask`: (Linear) Multi-task linear regressors with variable selection. These estimators fit multiple regression problems (or tasks) jointly, while inducing sparse coefficients. While the inferred coefficients may differ between the tasks, they are constrained to agree on the features that are selected (non-zero coefficients).\n",
    "                - MultiTaskElasticNet([alpha, ...]) Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\n",
    "                - MultiTaskElasticNetCV(*[, ...]) Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
    "                - MultiTaskLasso([alpha, ...]) Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
    "                - MultiTaskLassoCV(*[, eps, ...]) Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
    "\n",
    "            - `outlier-robust`: (Linear) Outlier-robust regressors. Any estimator using the Huber loss would also be robust to outliers, e.g. SGDRegressor with loss='huber'.\n",
    "                - HuberRegressor(*[, epsilon, ...]) Linear regression model that is robust to outliers.\n",
    "                - QuantileRegressor(*[, ...]) Linear regression model that predicts conditional quantiles.\n",
    "                - RANSACRegressor([...]) RANSAC (RANdom SAmple Consensus) algorithm.\n",
    "                - TheilSenRegressor(*[, ...]) Theil-Sen Estimator: robust multivariate regression model.\n",
    "\n",
    "            - `glm`: (Linear) Generalized linear models (GLM) for regression. These models allow for response variables to have error distributions other than a normal distribution\n",
    "                - PoissonRegressor(*[, alpha, ...])Generalized Linear Model with a Poisson distribution.\n",
    "                - TweedieRegressor(*[, power, ...])Generalized Linear Model with a Tweedie distribution.\n",
    "                - GammaRegressor(*[, alpha, ...])Generalized Linear Model with a Gamma distribution.\n",
    "\n",
    "            - `kernel_ridge`\n",
    "                - kernel_ridge.KernelRidge([alpha, kernel, ...])\n",
    "\n",
    "            - `isotonic`: Isotonic regression. IsotonicRegression produces a series of predictions y^ for the training data which are the closest to the targets y in terms of mean squared error. These predictions are interpolated for predicting to unseen data\n",
    "                - IsotonicRegression(*[, y_min, ...]) Isotonic regression model.\n",
    "      \n",
    "        \"\"\"\n",
    "        self.DEFAULT_MODELS = {\n",
    "            #tree\n",
    "            'DecisionTreeRegressor': ('tree',DecisionTreeRegressor()),\n",
    "            'ExtraTreeRegressor': ('tree',ExtraTreeRegressor()),\n",
    "            # svm\n",
    "            'LinearSVR': ('svm',LinearSVR()),\n",
    "            'NuSVR': ('svm',NuSVR()),\n",
    "            'SVR': ('svm',SVR()),\n",
    "            # neighbors\n",
    "            'KNeighborsRegressor': ('neighbors',KNeighborsRegressor()),\n",
    "            # ensemble\n",
    "            'AdaBoostRegressor': ('ensemble',AdaBoostRegressor()),\n",
    "            'ExtraTreesRegressor':('ensemble',ExtraTreesRegressor()),\n",
    "            'GradientBoostingRegressor':('ensemble',GradientBoostingRegressor()),\n",
    "            'RandomForestRegressor':('ensemble',RandomForestRegressor()),\n",
    "            # gaussian_process\n",
    "            'GaussianProcessRegressor': ('gaussian_process',GaussianProcessRegressor()),\n",
    "            # neural_network\n",
    "            'MLPRegressor': ('neural_network',MLPRegressor()),\n",
    "            # linear\n",
    "            'LinearRegression': ('linear',LinearRegression()),\n",
    "            'Ridge': ('linear',Ridge()),\n",
    "            'RidgeCV': ('linear',RidgeCV()),\n",
    "            'SGDRegressor': ('linear',SGDRegressor()),\n",
    "            # linear_variable_selection\n",
    "            'ElasticNet': ('linear_variable_selection',ElasticNet()),\n",
    "            'ElasticNetCV': ('linear_variable_selection',ElasticNetCV()),\n",
    "            'Lars': ('linear_variable_selection',Lars()),\n",
    "            'LarsCV': ('linear_variable_selection',LarsCV()),\n",
    "            'Lasso': ('linear_variable_selection',Lasso()),\n",
    "            'LassoCV': ('linear_variable_selection',LassoCV()),\n",
    "            'LassoLars': ('linear_variable_selection',LassoLars()),\n",
    "            'LassoLarsCV': ('linear_variable_selection',LassoLarsCV()),\n",
    "            'LassoLarsIC': ('linear_variable_selection',LassoLarsIC()),\n",
    "            'OrthogonalMatchingPursuit': ('linear_variable_selection',OrthogonalMatchingPursuit()),\n",
    "            'OrthogonalMatchingPursuitCV': ('linear_variable_selection',OrthogonalMatchingPursuitCV()),\n",
    "            # bayesian\n",
    "            'ARDRegression': ('bayesian',ARDRegression()),\n",
    "            'BayesianRidge': ('bayesian',BayesianRidge()),\n",
    "            # outlier_robust\n",
    "            'HuberRegressor': ('outlier_robust',HuberRegressor()),\n",
    "            'RANSACRegressor': ('outlier_robust',RANSACRegressor()),\n",
    "            'TheilSenRegressor': ('outlier_robust',TheilSenRegressor()),\n",
    "            # glm\n",
    "            'PoissonRegressor': ('glm',PoissonRegressor()),\n",
    "            'TweedieRegressor': ('glm',TweedieRegressor()),\n",
    "            'GammaRegressor': ('glm',GammaRegressor()),\n",
    "            # kernel_ridge\n",
    "            'KernelRidge': ('kernel_ridge',KernelRidge()),\n",
    "\n",
    "        }\n",
    "        self.verbose = verbose\n",
    "        self.__init_models__(models)\n",
    "        self.__filter_warnings__(ignore_warnings)\n",
    "\n",
    "    def score(self,X,y):\n",
    "        y_pred = self.predict(X)\n",
    "        scores = pd.DataFrame()\n",
    "        for (n,m),y_i in zip(self.models,y_pred):\n",
    "            iter_score = regression_scores(y,y_i)\n",
    "            iter_score[\"model\"] = type_of(m)\n",
    "            iter_score[\"params\"] = tryf_return(str({}),str(m.get_params))\n",
    "            iter_score[\"group\"] = n\n",
    "            scores = pd.concat([scores,pd.DataFrame([iter_score])])\n",
    "        scores = scores.set_index([\"model\",\"params\",\"group\"])\n",
    "        return scores.reset_index([\"model\",\"params\",\"group\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AutoRegressor(ignore_warnings=True)\n",
    "# y_pred = clf.fit_predict(X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.score(X_test,y_test).sort_values(by=\"R2\",ascending=False)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.cross_validate(X,y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.array([1,2,2,1],dtype=object))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = load_datasets(\"../../../datasets/other/wdbc/\")\n",
    "datasets = load_datasets(\"../../../datasets/brz_test14\",index=\"index\")\n",
    "datasets.y = datasets.y.iloc[:,-1]\n",
    "X_train,X_test,y_train,y_test = train_test_split(datasets.X,datasets.y,stratify=datasets.y, test_size=0.3, random_state=1)\n",
    "clf = AutoClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Fitted DecisionTreeClassifier in 00:00:00\n",
      ">> Fitted ExtraTreeClassifier in 00:00:00\n",
      "> Fitting LogisticRegression"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fitted LogisticRegression in 00:00:00\n",
      "> Fitting LogisticRegressionCV"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fitted LogisticRegressionCV in 00:00:12\n",
      ">> Fitted PassiveAggressiveClassifier in 00:00:00\n",
      ">> Fitted Perceptron in 00:00:00\n",
      ">> Fitted RidgeClassifier in 00:00:00\n",
      "> Fitting RidgeClassifierCV"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fitted RidgeClassifierCV in 00:00:00\n",
      ">> Fitted SGDClassifier in 00:00:00\n",
      "> Fitting LinearSVC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fitted LinearSVC in 00:00:04\n",
      ">> Fitted NuSVC in 00:00:00\n",
      ">> Fitted SVC in 00:00:00\n",
      ">> Fitted KNeighborsClassifier in 00:00:00\n",
      "> Fitting AdaBoostClassifier"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fitted AdaBoostClassifier in 00:00:00\n",
      ">> Fitted ExtraTreesClassifier in 00:00:00\n",
      ">> Fitted GradientBoostingClassifier in 00:00:04\n",
      ">> Fitted RandomForestClassifier in 00:00:00\n",
      ">> Fitted BernoulliNB in 00:00:00\n",
      ">> Fitted ComplementNB in 00:00:00\n",
      ">> Fitted GaussianNB in 00:00:00\n",
      ">> Fitted MultinomialNB in 00:00:00\n",
      ">> Fitted MLPClassifier in 00:00:03"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([array([0, 2, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 2, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       AttributeError(\"'NuSVC' object has no attribute 'shape_fit_'\"),\n",
       "       array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 3, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 2, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n",
       "       array([0, 3, 0, ..., 0, 0, 0]),\n",
       "       AttributeError(\"'ComplementNB' object has no attribute 'feature_log_prob_'\"),\n",
       "       array([0, 2, 0, ..., 0, 0, 0]),\n",
       "       AttributeError(\"'MultinomialNB' object has no attribute 'feature_log_prob_'\"),\n",
       "       array([0, 0, 0, ..., 0, 0, 0])], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.fit_predict(X_train,y_train,X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n",
      "/Applications/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>group</th>\n",
       "      <th>test_labels</th>\n",
       "      <th>test_counts</th>\n",
       "      <th>pred_counts</th>\n",
       "      <th>corr_counts</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>f1_binary</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tpr[0]</th>\n",
       "      <th>tpr[1]</th>\n",
       "      <th>tpr[2]</th>\n",
       "      <th>tpr[3]</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>tree</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1644, 22, 14, 14]</td>\n",
       "      <td>[1618, 3, 2, 2]</td>\n",
       "      <td>[[1618, 11, 6, 8], [13, 3, 5, 3], [9, 3, 2, 1]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.959268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959004</td>\n",
       "      <td>0.984784</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.959268</td>\n",
       "      <td>0.352446</td>\n",
       "      <td>0.351566</td>\n",
       "      <td>0.352446</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>tree</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1641, 23, 20, 10]</td>\n",
       "      <td>[1616, 8, 3, 1]</td>\n",
       "      <td>[[1616, 12, 7, 8], [11, 8, 5, 0], [9, 2, 3, 1]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961521</td>\n",
       "      <td>0.983567</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.400058</td>\n",
       "      <td>0.395648</td>\n",
       "      <td>0.400058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1694, 0, 0, 0]</td>\n",
       "      <td>[1643, 0, 0, 0]</td>\n",
       "      <td>[[1643, 0, 0, 0], [24, 0, 0, 0], [15, 0, 0, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.242473</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1694, 0, 0, 0]</td>\n",
       "      <td>[1643, 0, 0, 0]</td>\n",
       "      <td>[[1643, 0, 0, 0], [24, 0, 0, 0], [15, 0, 0, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.242473</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1677, 10, 3, 4]</td>\n",
       "      <td>[1639, 4, 2, 2]</td>\n",
       "      <td>[[1639, 3, 0, 1], [18, 4, 1, 1], [11, 2, 2, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964696</td>\n",
       "      <td>0.997565</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.972255</td>\n",
       "      <td>0.366058</td>\n",
       "      <td>0.636002</td>\n",
       "      <td>0.366058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1675, 19, 0, 0]</td>\n",
       "      <td>[1630, 3, 0, 0]</td>\n",
       "      <td>[[1630, 13, 0, 0], [21, 3, 0, 0], [13, 2, 0, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954916</td>\n",
       "      <td>0.992088</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963991</td>\n",
       "      <td>0.279272</td>\n",
       "      <td>0.282757</td>\n",
       "      <td>0.279272</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1671, 6, 11, 6]</td>\n",
       "      <td>[1637, 4, 2, 2]</td>\n",
       "      <td>[[1637, 0, 4, 2], [16, 4, 3, 1], [12, 0, 2, 1]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964902</td>\n",
       "      <td>0.996348</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.365754</td>\n",
       "      <td>0.540368</td>\n",
       "      <td>0.365754</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RidgeClassifierCV</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1673, 6, 11, 4]</td>\n",
       "      <td>[1637, 4, 2, 1]</td>\n",
       "      <td>[[1637, 0, 4, 2], [16, 4, 3, 1], [13, 0, 2, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963635</td>\n",
       "      <td>0.996348</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.970484</td>\n",
       "      <td>0.344920</td>\n",
       "      <td>0.519242</td>\n",
       "      <td>0.344920</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>linear</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1535, 19, 140, 0]</td>\n",
       "      <td>[1524, 3, 10, 0]</td>\n",
       "      <td>[[1524, 13, 106, 0], [5, 3, 16, 0], [3, 2, 10,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.907320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.933338</td>\n",
       "      <td>0.927572</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907320</td>\n",
       "      <td>0.429810</td>\n",
       "      <td>0.305539</td>\n",
       "      <td>0.429810</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>{}</td>\n",
       "      <td>svm</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1674, 7, 6, 7]</td>\n",
       "      <td>[1640, 4, 2, 3]</td>\n",
       "      <td>[[1640, 0, 1, 2], [16, 4, 2, 2], [11, 2, 2, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966655</td>\n",
       "      <td>0.998174</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.973436</td>\n",
       "      <td>0.387044</td>\n",
       "      <td>0.578256</td>\n",
       "      <td>0.387044</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NuSVC</td>\n",
       "      <td>{}</td>\n",
       "      <td>svm</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{}</td>\n",
       "      <td>svm</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1694, 0, 0, 0]</td>\n",
       "      <td>[1643, 0, 0, 0]</td>\n",
       "      <td>[[1643, 0, 0, 0], [24, 0, 0, 0], [15, 0, 0, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.242473</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>neighbors</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1676, 11, 5, 2]</td>\n",
       "      <td>[1638, 5, 2, 1]</td>\n",
       "      <td>[[1638, 3, 1, 1], [18, 5, 1, 0], [12, 1, 2, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964159</td>\n",
       "      <td>0.996957</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.971665</td>\n",
       "      <td>0.355489</td>\n",
       "      <td>0.582968</td>\n",
       "      <td>0.355489</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1655, 12, 20, 7]</td>\n",
       "      <td>[1627, 2, 2, 2]</td>\n",
       "      <td>[[1627, 4, 9, 3], [17, 2, 5, 0], [5, 6, 2, 2],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961031</td>\n",
       "      <td>0.990262</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.963991</td>\n",
       "      <td>0.343399</td>\n",
       "      <td>0.383866</td>\n",
       "      <td>0.343399</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1669, 11, 8, 6]</td>\n",
       "      <td>[1631, 4, 2, 2]</td>\n",
       "      <td>[[1631, 7, 2, 3], [16, 4, 3, 1], [13, 0, 2, 0]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.967532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961604</td>\n",
       "      <td>0.992696</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.967532</td>\n",
       "      <td>0.364841</td>\n",
       "      <td>0.481050</td>\n",
       "      <td>0.364841</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1663, 12, 10, 9]</td>\n",
       "      <td>[1633, 5, 3, 2]</td>\n",
       "      <td>[[1633, 1, 5, 4], [17, 5, 1, 1], [9, 1, 3, 2],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965569</td>\n",
       "      <td>0.993914</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.969894</td>\n",
       "      <td>0.392228</td>\n",
       "      <td>0.480212</td>\n",
       "      <td>0.392228</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1669, 8, 10, 7]</td>\n",
       "      <td>[1633, 2, 1, 4]</td>\n",
       "      <td>[[1633, 4, 4, 2], [18, 2, 4, 0], [12, 1, 1, 1]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961885</td>\n",
       "      <td>0.993914</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.968123</td>\n",
       "      <td>0.369312</td>\n",
       "      <td>0.474965</td>\n",
       "      <td>0.369312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>{}</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1545, 118, 20, 11]</td>\n",
       "      <td>[1533, 12, 4, 2]</td>\n",
       "      <td>[[1533, 99, 7, 4], [6, 12, 5, 1], [4, 3, 4, 4]...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938428</td>\n",
       "      <td>0.933049</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>0.466596</td>\n",
       "      <td>0.368937</td>\n",
       "      <td>0.466596</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>{}</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>{}</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1552, 39, 99, 4]</td>\n",
       "      <td>[1541, 3, 10, 0]</td>\n",
       "      <td>[[1541, 34, 65, 3], [5, 3, 16, 0], [3, 1, 10, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.917355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.937918</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917355</td>\n",
       "      <td>0.432396</td>\n",
       "      <td>0.292711</td>\n",
       "      <td>0.432396</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>{}</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>{}</td>\n",
       "      <td>neural_network</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[1643, 24, 15, 12]</td>\n",
       "      <td>[1650, 22, 9, 13]</td>\n",
       "      <td>[1625, 8, 2, 4]</td>\n",
       "      <td>[[1625, 9, 3, 6], [12, 8, 2, 2], [9, 3, 2, 1],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.967532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965899</td>\n",
       "      <td>0.989044</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.967532</td>\n",
       "      <td>0.447261</td>\n",
       "      <td>0.469600</td>\n",
       "      <td>0.447261</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model params           group   test_labels  \\\n",
       "0        DecisionTreeClassifier     {}            tree  [0, 1, 2, 3]   \n",
       "1           ExtraTreeClassifier     {}            tree  [0, 1, 2, 3]   \n",
       "2            LogisticRegression     {}          linear  [0, 1, 2, 3]   \n",
       "3          LogisticRegressionCV     {}          linear  [0, 1, 2, 3]   \n",
       "4   PassiveAggressiveClassifier     {}          linear  [0, 1, 2, 3]   \n",
       "5                    Perceptron     {}          linear  [0, 1, 2, 3]   \n",
       "6               RidgeClassifier     {}          linear  [0, 1, 2, 3]   \n",
       "7             RidgeClassifierCV     {}          linear  [0, 1, 2, 3]   \n",
       "8                 SGDClassifier     {}          linear  [0, 1, 2, 3]   \n",
       "9                     LinearSVC     {}             svm  [0, 1, 2, 3]   \n",
       "10                        NuSVC     {}             svm  [0, 1, 2, 3]   \n",
       "11                          SVC     {}             svm  [0, 1, 2, 3]   \n",
       "12         KNeighborsClassifier     {}       neighbors  [0, 1, 2, 3]   \n",
       "13           AdaBoostClassifier     {}        ensemble  [0, 1, 2, 3]   \n",
       "14         ExtraTreesClassifier     {}        ensemble  [0, 1, 2, 3]   \n",
       "15   GradientBoostingClassifier     {}        ensemble  [0, 1, 2, 3]   \n",
       "16       RandomForestClassifier     {}        ensemble  [0, 1, 2, 3]   \n",
       "17                  BernoulliNB     {}     naive_bayes  [0, 1, 2, 3]   \n",
       "18                 ComplementNB     {}     naive_bayes  [0, 1, 2, 3]   \n",
       "19                   GaussianNB     {}     naive_bayes  [0, 1, 2, 3]   \n",
       "20                MultinomialNB     {}     naive_bayes  [0, 1, 2, 3]   \n",
       "21                MLPClassifier     {}  neural_network  [0, 1, 2, 3]   \n",
       "\n",
       "           test_counts          pred_counts       corr_counts  \\\n",
       "0   [1643, 24, 15, 12]   [1644, 22, 14, 14]   [1618, 3, 2, 2]   \n",
       "1   [1643, 24, 15, 12]   [1641, 23, 20, 10]   [1616, 8, 3, 1]   \n",
       "2   [1643, 24, 15, 12]      [1694, 0, 0, 0]   [1643, 0, 0, 0]   \n",
       "3   [1643, 24, 15, 12]      [1694, 0, 0, 0]   [1643, 0, 0, 0]   \n",
       "4   [1643, 24, 15, 12]     [1677, 10, 3, 4]   [1639, 4, 2, 2]   \n",
       "5   [1643, 24, 15, 12]     [1675, 19, 0, 0]   [1630, 3, 0, 0]   \n",
       "6   [1643, 24, 15, 12]     [1671, 6, 11, 6]   [1637, 4, 2, 2]   \n",
       "7   [1643, 24, 15, 12]     [1673, 6, 11, 4]   [1637, 4, 2, 1]   \n",
       "8   [1643, 24, 15, 12]   [1535, 19, 140, 0]  [1524, 3, 10, 0]   \n",
       "9   [1643, 24, 15, 12]      [1674, 7, 6, 7]   [1640, 4, 2, 3]   \n",
       "10  [1643, 24, 15, 12]         [0, 0, 0, 0]               NaN   \n",
       "11  [1643, 24, 15, 12]      [1694, 0, 0, 0]   [1643, 0, 0, 0]   \n",
       "12  [1643, 24, 15, 12]     [1676, 11, 5, 2]   [1638, 5, 2, 1]   \n",
       "13  [1643, 24, 15, 12]    [1655, 12, 20, 7]   [1627, 2, 2, 2]   \n",
       "14  [1643, 24, 15, 12]     [1669, 11, 8, 6]   [1631, 4, 2, 2]   \n",
       "15  [1643, 24, 15, 12]    [1663, 12, 10, 9]   [1633, 5, 3, 2]   \n",
       "16  [1643, 24, 15, 12]     [1669, 8, 10, 7]   [1633, 2, 1, 4]   \n",
       "17  [1643, 24, 15, 12]  [1545, 118, 20, 11]  [1533, 12, 4, 2]   \n",
       "18  [1643, 24, 15, 12]         [0, 0, 0, 0]               NaN   \n",
       "19  [1643, 24, 15, 12]    [1552, 39, 99, 4]  [1541, 3, 10, 0]   \n",
       "20  [1643, 24, 15, 12]         [0, 0, 0, 0]               NaN   \n",
       "21  [1643, 24, 15, 12]    [1650, 22, 9, 13]   [1625, 8, 2, 4]   \n",
       "\n",
       "                                     confusion_matrix  f1_binary  f1_micro  \\\n",
       "0   [[1618, 11, 6, 8], [13, 3, 5, 3], [9, 3, 2, 1]...        NaN  0.959268   \n",
       "1   [[1616, 12, 7, 8], [11, 8, 5, 0], [9, 2, 3, 1]...        NaN  0.961039   \n",
       "2   [[1643, 0, 0, 0], [24, 0, 0, 0], [15, 0, 0, 0]...        NaN  0.969894   \n",
       "3   [[1643, 0, 0, 0], [24, 0, 0, 0], [15, 0, 0, 0]...        NaN  0.969894   \n",
       "4   [[1639, 3, 0, 1], [18, 4, 1, 1], [11, 2, 2, 0]...        NaN  0.972255   \n",
       "5   [[1630, 13, 0, 0], [21, 3, 0, 0], [13, 2, 0, 0...        NaN  0.963991   \n",
       "6   [[1637, 0, 4, 2], [16, 4, 3, 1], [12, 0, 2, 1]...        NaN  0.971074   \n",
       "7   [[1637, 0, 4, 2], [16, 4, 3, 1], [13, 0, 2, 0]...        NaN  0.970484   \n",
       "8   [[1524, 13, 106, 0], [5, 3, 16, 0], [3, 2, 10,...        NaN  0.907320   \n",
       "9   [[1640, 0, 1, 2], [16, 4, 2, 2], [11, 2, 2, 0]...        NaN  0.973436   \n",
       "10                                                NaN        NaN       NaN   \n",
       "11  [[1643, 0, 0, 0], [24, 0, 0, 0], [15, 0, 0, 0]...        NaN  0.969894   \n",
       "12  [[1638, 3, 1, 1], [18, 5, 1, 0], [12, 1, 2, 0]...        NaN  0.971665   \n",
       "13  [[1627, 4, 9, 3], [17, 2, 5, 0], [5, 6, 2, 2],...        NaN  0.963991   \n",
       "14  [[1631, 7, 2, 3], [16, 4, 3, 1], [13, 0, 2, 0]...        NaN  0.967532   \n",
       "15  [[1633, 1, 5, 4], [17, 5, 1, 1], [9, 1, 3, 2],...        NaN  0.969894   \n",
       "16  [[1633, 4, 4, 2], [18, 2, 4, 0], [12, 1, 1, 1]...        NaN  0.968123   \n",
       "17  [[1533, 99, 7, 4], [6, 12, 5, 1], [4, 3, 4, 4]...        NaN  0.915584   \n",
       "18                                                NaN        NaN       NaN   \n",
       "19  [[1541, 34, 65, 3], [5, 3, 16, 0], [3, 1, 10, ...        NaN  0.917355   \n",
       "20                                                NaN        NaN       NaN   \n",
       "21  [[1625, 9, 3, 6], [12, 8, 2, 2], [9, 3, 2, 1],...        NaN  0.967532   \n",
       "\n",
       "    ...  f1_weighted    tpr[0]    tpr[1]    tpr[2]    tpr[3]  accuracy  \\\n",
       "0   ...     0.959004  0.984784  0.125000  0.133333  0.166667  0.959268   \n",
       "1   ...     0.961521  0.983567  0.333333  0.200000  0.083333  0.961039   \n",
       "2   ...     0.955071  1.000000  0.000000  0.000000  0.000000  0.969894   \n",
       "3   ...     0.955071  1.000000  0.000000  0.000000  0.000000  0.969894   \n",
       "4   ...     0.964696  0.997565  0.166667  0.133333  0.166667  0.972255   \n",
       "5   ...     0.954916  0.992088  0.125000  0.000000  0.000000  0.963991   \n",
       "6   ...     0.964902  0.996348  0.166667  0.133333  0.166667  0.971074   \n",
       "7   ...     0.963635  0.996348  0.166667  0.133333  0.083333  0.970484   \n",
       "8   ...     0.933338  0.927572  0.125000  0.666667  0.000000  0.907320   \n",
       "9   ...     0.966655  0.998174  0.166667  0.133333  0.250000  0.973436   \n",
       "10  ...          NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11  ...     0.955071  1.000000  0.000000  0.000000  0.000000  0.969894   \n",
       "12  ...     0.964159  0.996957  0.208333  0.133333  0.083333  0.971665   \n",
       "13  ...     0.961031  0.990262  0.083333  0.133333  0.166667  0.963991   \n",
       "14  ...     0.961604  0.992696  0.166667  0.133333  0.166667  0.967532   \n",
       "15  ...     0.965569  0.993914  0.208333  0.200000  0.166667  0.969894   \n",
       "16  ...     0.961885  0.993914  0.083333  0.066667  0.333333  0.968123   \n",
       "17  ...     0.938428  0.933049  0.500000  0.266667  0.166667  0.915584   \n",
       "18  ...          NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "19  ...     0.938494  0.937918  0.125000  0.666667  0.000000  0.917355   \n",
       "20  ...          NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "21  ...     0.965899  0.989044  0.333333  0.133333  0.333333  0.967532   \n",
       "\n",
       "    balanced_accuracy  precision_macro  recall_macro  auc  \n",
       "0            0.352446         0.351566      0.352446  NaN  \n",
       "1            0.400058         0.395648      0.400058  NaN  \n",
       "2            0.250000         0.242473      0.250000  NaN  \n",
       "3            0.250000         0.242473      0.250000  NaN  \n",
       "4            0.366058         0.636002      0.366058  NaN  \n",
       "5            0.279272         0.282757      0.279272  NaN  \n",
       "6            0.365754         0.540368      0.365754  NaN  \n",
       "7            0.344920         0.519242      0.344920  NaN  \n",
       "8            0.429810         0.305539      0.429810  NaN  \n",
       "9            0.387044         0.578256      0.387044  NaN  \n",
       "10                NaN              NaN           NaN  NaN  \n",
       "11           0.250000         0.242473      0.250000  NaN  \n",
       "12           0.355489         0.582968      0.355489  NaN  \n",
       "13           0.343399         0.383866      0.343399  NaN  \n",
       "14           0.364841         0.481050      0.364841  NaN  \n",
       "15           0.392228         0.480212      0.392228  NaN  \n",
       "16           0.369312         0.474965      0.369312  NaN  \n",
       "17           0.466596         0.368937      0.466596  NaN  \n",
       "18                NaN              NaN           NaN  NaN  \n",
       "19           0.432396         0.292711      0.432396  NaN  \n",
       "20                NaN              NaN           NaN  NaN  \n",
       "21           0.447261         0.469600      0.447261  NaN  \n",
       "\n",
       "[22 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = clf.score(X_test,y_test,catch=False)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter1(df,keys,vals):\n",
    "    index_name = isNone(df.index.name,then=\"index\",els=df.index.name)\n",
    "    res = df.reset_index().set_index(keys)\n",
    "    res = res.loc[res.index.intersection(vals)]\n",
    "    if isinstance(res,pd.Series):\n",
    "        res = res.to_frame().T\n",
    "    res = res.reset_index().set_index(index_name)\n",
    "    res.index.name = df.index.name\n",
    "    return res\n",
    "\n",
    "filter1(df,\"model\",[\"ComplementNB\",\"MultinomialNB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scores\n",
    "df.drop(filter1(df,\"model\",[\"X\",\"ComplementNB\",\"MultinomialNB\"]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(scores,levels=['group'],ignore=['params'],action='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.cross_validate(datasets.X,datasets.y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df,levels=[],values=[],ignore=[],action=\"mean\"):\n",
    "    df_id = df.set_index(levels) if levels else df\n",
    "    indexes = df_id.index.drop_duplicates()\n",
    "    agg_scores = []\n",
    "    for idx in indexes:\n",
    "        agg_scores_i = agg(df_id.loc[idx],values=values,ignore=ignore,action=action)\n",
    "        agg_scores.append(agg_scores_i)\n",
    "    agg_scores = pd.DataFrame(agg_scores,index=indexes).reset_index() if levels else pd.DataFrame(agg_scores,index=indexes)\n",
    "    return agg_scores\n",
    "\n",
    "def agg(df,values=[],ignore=[],action=\"mean\"):\n",
    "    is_single_series = lambda x: isinstance(x,pd.Series)\n",
    "    if is_single_series(df):\n",
    "        agg_vals =  dict(df.drop(ignore))\n",
    "        agg_vals[\"count\"] = 1\n",
    "    else:\n",
    "        is_num_series = lambda x: type_of(x[0]).startswith((\"int\",\"float\"))\n",
    "        is_ndarray_series = lambda x: type_of(x[0]).startswith((\"ndarray\"))\n",
    "        values = pd.Index(as_iterable(values)).drop(ignore) if values else df.columns.drop(ignore)\n",
    "        f = {\n",
    "            \"mean\": lambda x: np.mean(x.dropna(),axis=0),\n",
    "            \"median\": lambda x: np.median(x.dropna(),axis=0),\n",
    "            \"sum\"  : lambda x: np.sum(x.dropna(),axis=0)\n",
    "        }[action]\n",
    "        agg_vals = dict()\n",
    "        for c in values:\n",
    "            if is_num_series(df[c]):\n",
    "                val = f(df[c])\n",
    "                agg_vals[c] = np.round(val,4)\n",
    "            elif is_ndarray_series(df[c]):\n",
    "                val = np.array(f(expand(df[c].dropna())))\n",
    "                agg_vals[c] = np.round(val,1)\n",
    "            else:\n",
    "                agg_vals[c] = np.unique(df[c])\n",
    "        agg_vals[\"count\"] = len(df)\n",
    "    agg_vals[\"action\"] = action\n",
    "    return agg_vals\n",
    "\n",
    "aggregate(scores,levels=['group'],ignore=['params'],action='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.score(X_test,y_test).sort_values(by=\"f1_binary\",ascending=False)\n",
    "scores\n",
    "# sns.scatterplot(scores[\"f1_binary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=scores,x=\"f1_binary\",y=\"accuracy\",hue=\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1,\"b\":2, \"c\":\"Hello\"}\n",
    "str_repr = lambda x: f\"'{str(x)}'\" if isinstance(x,str) else str(x) # Add '' around a string when converting it using str\n",
    "arg_repr = lambda x: f\"({', '.join(apply(list(x.items()),lambda i,v: '='.join([str(v[0]),str_repr(v[1])])))})\" # Represent a dict as (key=value,...) format\n",
    "arg_repr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AutoModelTuner:\n",
    "#     def __init__(self,models=None,verbose=True,ignore_warnings=False):\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cea0b2ba1d4e863dfb586909f3baf8988d507bc1f42173fc816b8e84176b62f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
